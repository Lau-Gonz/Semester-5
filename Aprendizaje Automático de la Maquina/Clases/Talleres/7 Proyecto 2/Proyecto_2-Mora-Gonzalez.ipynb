{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 Sentimientos - Machine Learning.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Integrantes:\n",
    "\n",
    "- Diryon Yonith Mora Romero.\n",
    "- Laura Valentina Gonzalez Rodriguez.\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Carga de Liberias Utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import f_classif, SelectFwe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Descripción del Dataset.\n",
    "\n",
    "El conjunto de datos que se va a utilizar para la clasificación de sentimientos de señales de voz es un conjunto de atributos extraídos de grabaciones de voz de personas que han hablado en un estado emocional de enojo, tristeza o felicidad. Cada grabación de voz se ha sometido a un proceso de análisis y se han extraído 18 atributos que describen diferentes características de la señal acústica, explicada brevemente después. El objetivo del proyecto es utilizar una red neuronal para clasificar estas señales de voz según el sentimiento que transmiten: enojo, tristeza o felicidad. Se espera que la red neuronal pueda aprender a identificar patrones y relaciones en los datos que permitan una clasificación precisa y eficiente de las señales de voz en función de su estado emocional.\n",
    "\n",
    "1. **meanfreq**: La frecuencia media (en kHz).\n",
    "1. **sd**: La desviación estándar de la frecuencia.\n",
    "1. **median**: La mediana de la frecuencia (en kHz).\n",
    "1. **Q25**: El primer cuartil o Q1 de la frecuencia (en kHz).\n",
    "1. **Q75**: El tercer cuartil o Q3 de la frecuencia (en kHz).\n",
    "1. **IQR**: El rango intercuartílico (en kHz).\n",
    "1. **skew**: Medida de la asimetría o el grado de distorsión de la distribución normal.\n",
    "1. **kurt**: Medida estadística que determina qué tanto varían las colas de la distribución de las de una distribución normal.\n",
    "1. **sp.ent**: Entropía espectral. Es una medida de irregularidad en la señal.\n",
    "1. **sfm**: The spectral flatness o coeficiente de tonalidad. Una medida en decibeles que caracteriza el tono de un sonido.\n",
    "1. **mode**: La moda en la frecuencia.\n",
    "1. **centroid**: El centroide espectral, indica en dónde está ubicado el centro de masa del espectro.\n",
    "1. **meanfun**: El promedio de la frecuencia fundamental medida a lo largo de la señal acústica.\n",
    "1. **minfun**: El mínimo de la frecuencia fundamental medida a lo largo de la señal acústica.\n",
    "1. **maxfun**: El máximo de la frecuencia fundamental medida a lo largo de la señal acústica.\n",
    "1. **meandom**: El promedio de la frecuencia dominante medida a lo largo de la señal acústica.\n",
    "1. **mindom**: El mínimo de la frecuencia dominante medida a lo largo de la señal acústica.\n",
    "1. **maxdom**: El máximo de la frecuencia dominante medida a lo largo de la señal acústica.\n",
    "1. **dfrange**: El rango de la frecuencia dominante medida a lo largo de la señal acústica.\n",
    "1. **modindx**: El índice de modulación.\n",
    "1. **lables**: El sentimiento asociado a la señal de voz, que puede ser enojado, triste o feliz."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Preparación de las Variables.\n",
    "\n",
    "En el conjunto de datos proporcionado, se han identificado tres columnas que parecen ser atributos de tipo índice, a saber, **Column1**, **Unnamed: 0** y **X**. Estas columnas no parecen aportar información relevante para el problema de clasificación planteado, por lo que se ha tomado la decisión de eliminarlas del conjunto de datos antes de realizar el análisis y modelado de los datos. Al eliminar estos atributos, se simplificará la tarea de clasificación y se reducirá el ruido en los datos, lo que podría resultar en una mayor precisión de los modelos de aprendizaje automático utilizados. Además, la eliminación de estos atributos no afectará la interpretación de los resultados ni la comprensión del problema en cuestión, ya que se trata de atributos que no tienen un significado intrínseco o relevancia conceptual en el contexto de las señales de voz y el análisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 21 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   meanfreq  909 non-null    float64\n",
      " 1   sd        909 non-null    float64\n",
      " 2   median    909 non-null    float64\n",
      " 3   Q25       909 non-null    float64\n",
      " 4   Q75       909 non-null    float64\n",
      " 5   IQR       909 non-null    float64\n",
      " 6   skew      909 non-null    float64\n",
      " 7   kurt      909 non-null    float64\n",
      " 8   sp.ent    909 non-null    float64\n",
      " 9   sfm       909 non-null    float64\n",
      " 10  mode      909 non-null    float64\n",
      " 11  centroid  909 non-null    float64\n",
      " 12  meanfun   909 non-null    float64\n",
      " 13  minfun    909 non-null    float64\n",
      " 14  maxfun    909 non-null    float64\n",
      " 15  meandom   909 non-null    float64\n",
      " 16  mindom    909 non-null    float64\n",
      " 17  maxdom    909 non-null    float64\n",
      " 18  dfrange   909 non-null    float64\n",
      " 19  modindx   909 non-null    float64\n",
      " 20  label     909 non-null    object \n",
      "dtypes: float64(20), object(1)\n",
      "memory usage: 149.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"emotions_by_voice registers.csv\")\n",
    "df = df.drop(df.columns[[0, 1, 2]], axis=1)\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0  0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1  0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2  0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3  0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4  0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "\n",
       "       kurt    sp.ent       sfm  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586    sad  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983    sad  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783    sad  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910    sad  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777    sad  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder utilizar una red neuronal y predecir valores categóricos, es necesario convertir la variable objetivo en variables numericas mediante la técnica de \"Label Encoding\". En este caso, se ajusta un codificador a los valores únicos de una columna de un marco de datos, y luego se transforma la columna con el codificador para asignar un valor numérico único a cada etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.869088</td>\n",
       "      <td>2.863717</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>1.191767</td>\n",
       "      <td>3.878650</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>1.312690</td>\n",
       "      <td>4.589995</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>1.096409</td>\n",
       "      <td>3.680995</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>1.386837</td>\n",
       "      <td>5.031744</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR      skew  \\\n",
       "0  0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.869088   \n",
       "1  0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  1.191767   \n",
       "2  0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  1.312690   \n",
       "3  0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  1.096409   \n",
       "4  0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  1.386837   \n",
       "\n",
       "       kurt    sp.ent       sfm  ...  centroid   meanfun    minfun    maxfun  \\\n",
       "0  2.863717  0.923566  0.307220  ...  0.181338  0.137742  0.023022  0.271186   \n",
       "1  3.878650  0.918848  0.298859  ...  0.186897  0.121811  0.018412  0.271186   \n",
       "2  4.589995  0.919519  0.313069  ...  0.189102  0.123758  0.083333  0.262295   \n",
       "3  3.680995  0.921361  0.329295  ...  0.183036  0.128469  0.044693  0.258065   \n",
       "4  5.031744  0.926238  0.337047  ...  0.168793  0.109720  0.022472  0.235294   \n",
       "\n",
       "    meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.777344  0.085938  6.226562  6.140625  0.116586      2  \n",
       "1  0.930339  0.085938  4.000000  3.914062  0.144983      2  \n",
       "2  0.332386  0.085938  0.625000  0.539062  0.334783      2  \n",
       "3  1.012019  0.085938  5.468750  5.382812  0.304910      2  \n",
       "4  0.228795  0.093750  0.750000  0.656250  0.306777      2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"label\"\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df[target])\n",
    "\n",
    "df[target] = le.transform(df[target])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la identificación de outliers y su respectiva imputación con SimpleImputer, pues estos valores atípicos pueden sesgar los resultados del análisis y afectar la calidad del modelo. En el código presentado, se realiza la identificación y posterior imputación de outliers en los datos utilizando la técnica de imputación de valores faltantes K-Vecinos más Cercanos (KNNImputer) y la prueba de valores Z para la detección de outliers.\n",
    "\n",
    "Primero, se utiliza la prueba F para seleccionar las características más significativas del conjunto de datos, lo que puede ayudar a mejorar la eficiencia de la detección de outliers y la imputación. Luego, se define una función llamada imp_feature_outliers(df) que utiliza la prueba de valores Z para detectar outliers en cada característica y reemplaza los valores atípicos con NaN. Posteriormente, se utiliza la técnica KNNImputer para imputar los valores faltantes en cada columna NaN utilizando los valores de los K-vecinos más cercanos. Finalmente, se devuelve un DataFrame con los valores imputados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>mode</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.187476</td>\n",
       "      <td>0.126197</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>0.216901</td>\n",
       "      <td>0.181338</td>\n",
       "      <td>0.137742</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>6.226562</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>0.116586</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.195070</td>\n",
       "      <td>0.130847</td>\n",
       "      <td>0.243987</td>\n",
       "      <td>0.113140</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.298859</td>\n",
       "      <td>0.135648</td>\n",
       "      <td>0.186897</td>\n",
       "      <td>0.121811</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.914062</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.204945</td>\n",
       "      <td>0.131422</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.118556</td>\n",
       "      <td>0.919519</td>\n",
       "      <td>0.313069</td>\n",
       "      <td>0.264070</td>\n",
       "      <td>0.189102</td>\n",
       "      <td>0.123758</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.332386</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.539062</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.174115</td>\n",
       "      <td>0.129949</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.107017</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>0.152032</td>\n",
       "      <td>0.183036</td>\n",
       "      <td>0.128469</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1.012019</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>5.468750</td>\n",
       "      <td>5.382812</td>\n",
       "      <td>0.304910</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.116783</td>\n",
       "      <td>0.216326</td>\n",
       "      <td>0.099543</td>\n",
       "      <td>0.926238</td>\n",
       "      <td>0.337047</td>\n",
       "      <td>0.153764</td>\n",
       "      <td>0.168793</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228795</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.306777</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR    sp.ent  \\\n",
       "0  0.181338  0.060495  0.187476  0.126197  0.233586  0.107389  0.923566   \n",
       "1  0.186897  0.062260  0.195070  0.130847  0.243987  0.113140  0.918848   \n",
       "2  0.189102  0.062901  0.204945  0.131422  0.249978  0.118556  0.919519   \n",
       "3  0.183036  0.060051  0.174115  0.129949  0.236967  0.107017  0.921361   \n",
       "4  0.168793  0.057910  0.156266  0.116783  0.216326  0.099543  0.926238   \n",
       "\n",
       "        sfm      mode  centroid   meanfun    maxfun   meandom    mindom  \\\n",
       "0  0.307220  0.216901  0.181338  0.137742  0.271186  0.777344  0.085938   \n",
       "1  0.298859  0.135648  0.186897  0.121811  0.271186  0.930339  0.085938   \n",
       "2  0.313069  0.264070  0.189102  0.123758  0.262295  0.332386  0.085938   \n",
       "3  0.329295  0.152032  0.183036  0.128469  0.258065  1.012019  0.085938   \n",
       "4  0.337047  0.153764  0.168793  0.109720  0.235294  0.228795  0.093750   \n",
       "\n",
       "     maxdom   dfrange   modindx  label  \n",
       "0  6.226562  6.140625  0.116586    2.0  \n",
       "1  4.000000  3.914062  0.144983    2.0  \n",
       "2  0.625000  0.539062  0.334783    2.0  \n",
       "3  5.468750  5.382812  0.304910    2.0  \n",
       "4  0.750000  0.656250  0.306777    2.0  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectFwe(f_classif, alpha = 0.05)\n",
    "selector.fit(df.drop(target, axis = 1), df[target])\n",
    "df = pd.DataFrame(selector.transform(df.drop(target, axis = 1)), columns=selector.get_feature_names_out()).join(df[target]) # type: ignore\n",
    "\n",
    "features = df.columns.drop(list(df.filter(regex=target)))\n",
    "def imp_feature_outliers(df):\n",
    "    for feature in features:\n",
    "        z_scores = np.abs(stats.zscore(df[feature]))\n",
    "        outliers = (z_scores > 3)\n",
    "        df.loc[outliers, feature] = np.nan\n",
    "    imp = KNNImputer(missing_values=np.nan, n_neighbors=10)\n",
    "    return pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "\n",
    "df = imp_feature_outliers(df)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de utilizar el conjunto de datos para entrenar la red neuronal, es importante estandarizar las variables. La estandarización es un proceso que transforma los valores de los atributos de tal manera que tengan una media de cero y una desviación estándar de uno. Esto ayuda a igualar la escala de los diferentes atributos, lo que es especialmente importante en redes neuronales que utilizan funciones de activación como la función sigmoide, que puede verse afectada por los valores de entrada en escalas muy diferentes. Además, puede ayudar a acelerar el entrenamiento de la red neuronal y mejorar la precisión de las predicciones. Al tener una escala más uniforme, los gradientes que se utilizan para ajustar los pesos de la red pueden ser más estables y converger más rápidamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    df[feature] = StandardScaler().fit_transform(df[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Conjuntos de Entrenamiento, Validación y Test.\n",
    "\n",
    "Es común en aprendizaje automático dividir el conjunto de datos en tres partes: entrenamiento, validación y prueba. La división adecuada de los datos puede ayudar a obtener un modelo más preciso y confiable. En general, se recomienda una división 60-20-20 para los datos de entrenamiento, validación y prueba, respectivamente. La razón detrás de esta proporción es que el modelo necesita una cantidad suficiente de datos de entrenamiento para aprender patrones y características de los datos, mientras que la validación se utiliza para ajustar los parámetros del modelo y evitar el sobreajuste. La prueba final permite evaluar el rendimiento del modelo en datos no vistos previamente y verificar su capacidad para generalizar y predecir correctamente. Por lo tanto, dividir los datos en estas proporciones puede ayudar a obtener un modelo más sólido y preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de conjunto de entrenamiento:  545\n",
      "Tamaño de conjunto de validación:  182\n",
      "Tamaño de conjunto de pruebas:  182\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[target]).values\n",
    "y = df[target].values\n",
    "\n",
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio / (test_ratio + validation_ratio), random_state=0)\n",
    "\n",
    "print(\"Tamaño de conjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Tamaño de conjunto de validación: \", len(X_val))\n",
    "print(\"Tamaño de conjunto de pruebas: \", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Preparación de Datos.\n",
    "\n",
    "El propósito es que dada la clase MyDataset, preparar los datos para ser alimentados a una red neuronal. La clase toma como entrada un dataframe y el nombre de la columna objetivo (variable dependiente). Luego, se crean tres objetos MyDataset para los conjuntos de entrenamiento, validación y prueba, respectivamente. Cada uno de ellos se crea pasando el subconjunto correspondiente del dataframe (*train*, *test* o *val*) y el nombre de la columna objetivo (*label*). De esta manera, cada objeto MyDataset contiene los datos de entrada y de salida correspondientes para su uso en el entrenamiento y evaluación de la red neuronal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super()\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y).type(torch.LongTensor) # type: ignore\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_sec = MyDataset(X_train, y_train)\n",
    "val_sec = MyDataset(X_val, y_val)\n",
    "test_sec = MyDataset(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen tres objetos de DataLoader en PyTorch que son utilizados para la carga de datos de entrenamiento, validación y prueba. Cada objeto es creado a partir de un conjunto de datos específico y tiene un tamaño de lote determinado. Estos objetos son utilizados para cargar los datos en el modelo de red neuronal y entrenar el modelo con diferentes lotes de datos en cada iteración. Además, se establece el número de trabajadores para el procesamiento de datos en paralelo y se define si los datos serán barajeados o no en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 0\n",
    "\n",
    "train_data = DataLoader(\n",
    "    train_sec,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    test_sec,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "val_data = DataLoader(\n",
    "    val_sec,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera un ejemplo de cómo utilizar el objeto DataLoader para cargar los datos de entrenamiento. En este caso, se utiliza un bucle for para recorrer cada lote de datos en el objeto train_data. Para cada lote, el bucle imprime las dimensiones del tensor de entrada y del tensor de salida, así como los valores de cada tensor. El uso de la función enumerate permite acceder al índice y al valor correspondiente de cada lote en el objeto train_data. En este caso, el índice i no se utiliza, pero la variable (data, labels) se utiliza para almacenar cada lote de datos y sus correspondientes etiquetas. Finalmente, la instrucción break se utiliza para detener el bucle después de imprimir el primer lote de datos, lo que es útil para comprobar rápidamente si la carga de datos se realizó correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 17]) torch.Size([4])\n",
      "tensor([[ 0.3617, -0.7446,  0.0619,  0.4637, -0.3518, -0.8011, -0.1475, -0.3920,\n",
      "         -0.1606,  0.3617,  0.2033, -0.1192,  1.9854,  0.4490,  1.3682,  1.3526,\n",
      "          0.1075],\n",
      "        [-1.7151,  1.3460, -1.7041, -0.9589, -2.1135, -0.0074,  0.7054,  0.4192,\n",
      "         -1.6123, -1.7151, -0.5242, -0.5891,  0.2476, -1.3066,  0.3388,  0.4066,\n",
      "          0.4161],\n",
      "        [ 0.8936, -1.1471,  0.4530,  0.8600,  0.5089, -0.8031, -0.9995, -1.1980,\n",
      "         -0.0347,  0.8936,  0.9152,  0.9177,  0.0691,  0.2096,  0.1481,  0.1392,\n",
      "         -1.1165],\n",
      "        [ 1.3775, -0.0717,  1.3506,  1.3544,  1.4198, -0.9020, -1.2586, -0.5111,\n",
      "          1.1181,  1.3775,  1.0330,  0.9177,  0.5461,  1.4066,  0.0130, -0.0564,\n",
      "         -0.1737]]) tensor([0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, labels) in enumerate(train_data):\n",
    "    print(data.shape, labels.shape)\n",
    "    print(data, labels)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Arquitectura de la Red Neuronal.\n",
    "\n",
    "A continuación, se definen dos clases de PyTorch que se utilizan en el entrenamiento y validación del modelo de redes neuronales para la clasificación de emociones en datos de voz. Estas son definidas como subclases de la clase base \"nn.Module\" de PyTorch, lo que permite utilizar todos los métodos y herramientas proporcionados por la biblioteca PyTorch.\n",
    "\n",
    "La primera clase es \"Net\", que define una red neuronal que consiste en una serie de capas ocultas y una capa de salida. El número de capas ocultas y su tamaño se especifican como argumentos de entrada. La clase utiliza una función de activación ReLU en las capas ocultas y una capa lineal en la capa de salida.\n",
    "\n",
    "La segunda clase es \"EnsembleModel\", que es una red neuronal que utiliza múltiples modelos \"Net\" y los fusiona en una sola salida. Esto se hace concatenando las salidas de los modelos individuales y alimentando la salida combinada a través de una capa lineal adicional. La salida final de la red es la predicción de la clase para la entrada dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_sizes: list[int], output_size: int):\n",
    "        super().__init__()\n",
    "        hidden_sizes = hidden_sizes.copy()\n",
    "        hidden_sizes.sort(reverse = True)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, output_size: int, models: list) -> None:\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        self.classifier = nn.Linear(output_size * len(models), output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        X = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            X.append(model(x))\n",
    "            X[-1] = F.sigmoid(X[-1])\n",
    "\n",
    "        x = torch.cat(X, dim = 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se definen las dimensiones de entrada, salida y las capas ocultas de la red neuronal. La variable *input_size* se define como la longitud de la entrada X, mientras que *output_size* se define como el número de clases únicas en la variable objetivo. La lista *hidden_sizes* se define como una lista de dos enteros. El primer entero se define como 2/3 del tamaño de la entrada más el tamaño de la salida, mientras que el segundo entero se define como la suma de la entrada y la salida dividida por 2. Estos valores se utilizan para definir las dimensiones de las capas ocultas de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:  17\n",
      "output_size:  3\n",
      "hidden_sizes:  [14, 10]\n"
     ]
    }
   ],
   "source": [
    "input_size = len(X[0])\n",
    "output_size = len(le.classes_)\n",
    "hidden_sizes = [2 * input_size // 3 + output_size, (input_size + output_size) >> 1]\n",
    "print('input_size: ', input_size)\n",
    "print('output_size: ', output_size)\n",
    "print('hidden_sizes: ', hidden_sizes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Definición del Optimizador, Función de Costo y Modelo.\n",
    "\n",
    "Finalmente, se define la función *train_model* que toma como argumentos un modelo de PyTorch, dos dataloaders (train_loader y valid_loader) y un número de épocas num_epochs. El optimizador *Adam* es utilizado con una tasa de aprendizaje de 1e-3 y una decaída de peso de 1e-4. La función de pérdida usada es la *CrossEntropyLoss* y un planificador de tasa de aprendizaje *CosineAnnealingLR* es utilizado para reducir la tasa de aprendizaje a la mitad después de un máximo de 32 épocas. El modelo se entrena en modo de entrenamiento y se evalúa en modo de evaluación en cada época. La función de pérdida y la exactitud en el conjunto de validación son calculados en cada época. El modelo con menor pérdida en el conjunto de validación es guardado y retornado al final del entrenamiento.\n",
    "\n",
    "Note que, Adam es un algoritmo de optimización de descenso de gradiente estocástico que utiliza estimaciones adaptativas del primer y segundo momento de los gradientes para actualizar los pesos de la red neuronal. A diferencia del SGD, la tasa de aprendizaje se adapta de forma adaptativa según las estimaciones del momento, lo que puede permitir una convergencia más rápida y menos sensible a los hiperparámetros de la tasa de aprendizaje y el momento. Además, la función de pérdida Mean Squared Error (MSE) se utiliza en problemas de regresión, mientras que Cross Entropy Loss (CEL) se utiliza en problemas de clasificación.  En cuanto al planificador de tasa de aprendizaje CosineAnnealingLR, su objetivo es disminuir gradualmente la tasa de aprendizaje del modelo a medida que se avanza en las épocas de entrenamiento, útil para evitar que el modelo se atasque en mínimos locales y ayuda a encontrar el mínimo global de la función de pérdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"emotions_by_voice.registers.pt\"\n",
    "def train_model(model, train_loader, valid_loader, num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 32, eta_min=1e-4)\n",
    "\n",
    "    lowest_val_loss = np.inf\n",
    "    best_model = model\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total = 0\n",
    "        with torch.enable_grad():\n",
    "            for data, target in train_loader:\n",
    "                batch_size = target.size(0)\n",
    "                outputs = model(data)\n",
    "                loss = loss_fn(outputs, target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += batch_size * loss.item()\n",
    "                total += batch_size\n",
    "        train_loss /= total\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                outputs = model(data)\n",
    "\n",
    "                preds = outputs.argmax(dim = 1)\n",
    "\n",
    "                batch_size = target.size(0)\n",
    "                total += batch_size\n",
    "                correct += (preds == target).float().sum().item()\n",
    "\n",
    "                loss = loss_fn(outputs, target)\n",
    "                valid_loss += batch_size * loss.item()\n",
    "        valid_loss /= total\n",
    "        acc = correct / total\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.6f}\".format(\n",
    "                i, train_loss, valid_loss, acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if valid_loss < lowest_val_loss:\n",
    "            print(\n",
    "                \"Better loss found. ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                    lowest_val_loss, valid_loss\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), MODEL_FILENAME)\n",
    "            lowest_val_loss = valid_loss\n",
    "            best_model = deepcopy(model)\n",
    "    return best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado todo lo anterior, se hace la creación de una red neuronal de tipo Ensemble, donde se combinan las predicciones de varias redes neuronales independientes. Primero se define el número de épocas a entrenar y el número de modelos a utilizar. Luego, se crean los modelos individuales con la clase Net y se entrenan con la función train_model. Después se crea el modelo Ensemble con la clase EnsembleModel y se entrena de nuevo con la función train_model. Finalmente, el modelo entrenado se devuelve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.091917 \tValidation Loss: 1.085374 \tAccuracy: 0.318681\n",
      "Better loss found. (inf --> 1.085374).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.049196 \tValidation Loss: 1.041527 \tAccuracy: 0.351648\n",
      "Better loss found. (1.085374 --> 1.041527).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.001625 \tValidation Loss: 1.001629 \tAccuracy: 0.483516\n",
      "Better loss found. (1.041527 --> 1.001629).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.959847 \tValidation Loss: 0.974555 \tAccuracy: 0.494505\n",
      "Better loss found. (1.001629 --> 0.974555).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.926800 \tValidation Loss: 0.958685 \tAccuracy: 0.516484\n",
      "Better loss found. (0.974555 --> 0.958685).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.902375 \tValidation Loss: 0.944711 \tAccuracy: 0.532967\n",
      "Better loss found. (0.958685 --> 0.944711).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.884261 \tValidation Loss: 0.934004 \tAccuracy: 0.549451\n",
      "Better loss found. (0.944711 --> 0.934004).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.869335 \tValidation Loss: 0.926322 \tAccuracy: 0.554945\n",
      "Better loss found. (0.934004 --> 0.926322).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.857343 \tValidation Loss: 0.920406 \tAccuracy: 0.565934\n",
      "Better loss found. (0.926322 --> 0.920406).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.847416 \tValidation Loss: 0.914973 \tAccuracy: 0.560440\n",
      "Better loss found. (0.920406 --> 0.914973).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.838665 \tValidation Loss: 0.911392 \tAccuracy: 0.571429\n",
      "Better loss found. (0.914973 --> 0.911392).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.831266 \tValidation Loss: 0.908339 \tAccuracy: 0.576923\n",
      "Better loss found. (0.911392 --> 0.908339).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.824686 \tValidation Loss: 0.905781 \tAccuracy: 0.571429\n",
      "Better loss found. (0.908339 --> 0.905781).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.818215 \tValidation Loss: 0.902744 \tAccuracy: 0.582418\n",
      "Better loss found. (0.905781 --> 0.902744).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.813005 \tValidation Loss: 0.899243 \tAccuracy: 0.576923\n",
      "Better loss found. (0.902744 --> 0.899243).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.807960 \tValidation Loss: 0.896514 \tAccuracy: 0.582418\n",
      "Better loss found. (0.899243 --> 0.896514).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.803582 \tValidation Loss: 0.894505 \tAccuracy: 0.587912\n",
      "Better loss found. (0.896514 --> 0.894505).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.799546 \tValidation Loss: 0.893171 \tAccuracy: 0.593407\n",
      "Better loss found. (0.894505 --> 0.893171).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.795691 \tValidation Loss: 0.891996 \tAccuracy: 0.582418\n",
      "Better loss found. (0.893171 --> 0.891996).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.792361 \tValidation Loss: 0.891254 \tAccuracy: 0.593407\n",
      "Better loss found. (0.891996 --> 0.891254).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.789259 \tValidation Loss: 0.890490 \tAccuracy: 0.593407\n",
      "Better loss found. (0.891254 --> 0.890490).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.786510 \tValidation Loss: 0.889913 \tAccuracy: 0.587912\n",
      "Better loss found. (0.890490 --> 0.889913).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.783883 \tValidation Loss: 0.889399 \tAccuracy: 0.587912\n",
      "Better loss found. (0.889913 --> 0.889399).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.781703 \tValidation Loss: 0.888946 \tAccuracy: 0.582418\n",
      "Better loss found. (0.889399 --> 0.888946).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.779669 \tValidation Loss: 0.888730 \tAccuracy: 0.587912\n",
      "Better loss found. (0.888946 --> 0.888730).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.777921 \tValidation Loss: 0.888652 \tAccuracy: 0.587912\n",
      "Better loss found. (0.888730 --> 0.888652).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.776408 \tValidation Loss: 0.888360 \tAccuracy: 0.587912\n",
      "Better loss found. (0.888652 --> 0.888360).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.775106 \tValidation Loss: 0.888226 \tAccuracy: 0.582418\n",
      "Better loss found. (0.888360 --> 0.888226).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.774001 \tValidation Loss: 0.888114 \tAccuracy: 0.576923\n",
      "Better loss found. (0.888226 --> 0.888114).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.773031 \tValidation Loss: 0.888139 \tAccuracy: 0.571429\n",
      "Epoch: 30 \tTraining Loss: 0.772205 \tValidation Loss: 0.888092 \tAccuracy: 0.571429\n",
      "Better loss found. (0.888114 --> 0.888092).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.771532 \tValidation Loss: 0.888126 \tAccuracy: 0.571429\n",
      "Epoch: 32 \tTraining Loss: 0.770933 \tValidation Loss: 0.888148 \tAccuracy: 0.571429\n",
      "Epoch: 33 \tTraining Loss: 0.770420 \tValidation Loss: 0.888139 \tAccuracy: 0.571429\n",
      "Epoch: 34 \tTraining Loss: 0.769956 \tValidation Loss: 0.888209 \tAccuracy: 0.571429\n",
      "Epoch: 35 \tTraining Loss: 0.769544 \tValidation Loss: 0.888268 \tAccuracy: 0.571429\n",
      "Epoch: 36 \tTraining Loss: 0.769082 \tValidation Loss: 0.888412 \tAccuracy: 0.571429\n",
      "Epoch: 37 \tTraining Loss: 0.768620 \tValidation Loss: 0.888595 \tAccuracy: 0.576923\n",
      "Epoch: 38 \tTraining Loss: 0.768095 \tValidation Loss: 0.888857 \tAccuracy: 0.576923\n",
      "Epoch: 39 \tTraining Loss: 0.767555 \tValidation Loss: 0.889066 \tAccuracy: 0.582418\n",
      "Epoch: 40 \tTraining Loss: 0.766927 \tValidation Loss: 0.889368 \tAccuracy: 0.576923\n",
      "Epoch: 41 \tTraining Loss: 0.766275 \tValidation Loss: 0.889612 \tAccuracy: 0.582418\n",
      "Epoch: 42 \tTraining Loss: 0.765426 \tValidation Loss: 0.889953 \tAccuracy: 0.582418\n",
      "Epoch: 43 \tTraining Loss: 0.764577 \tValidation Loss: 0.890583 \tAccuracy: 0.582418\n",
      "Epoch: 44 \tTraining Loss: 0.763403 \tValidation Loss: 0.891213 \tAccuracy: 0.571429\n",
      "Epoch: 45 \tTraining Loss: 0.762279 \tValidation Loss: 0.891857 \tAccuracy: 0.565934\n",
      "Epoch: 46 \tTraining Loss: 0.760680 \tValidation Loss: 0.892523 \tAccuracy: 0.560440\n",
      "Epoch: 47 \tTraining Loss: 0.759206 \tValidation Loss: 0.892827 \tAccuracy: 0.560440\n",
      "Epoch: 48 \tTraining Loss: 0.757310 \tValidation Loss: 0.893456 \tAccuracy: 0.554945\n",
      "Epoch: 49 \tTraining Loss: 0.755601 \tValidation Loss: 0.894652 \tAccuracy: 0.549451\n",
      "Epoch: 50 \tTraining Loss: 0.753654 \tValidation Loss: 0.895282 \tAccuracy: 0.543956\n",
      "Epoch: 51 \tTraining Loss: 0.751627 \tValidation Loss: 0.895306 \tAccuracy: 0.538462\n",
      "Epoch: 52 \tTraining Loss: 0.749370 \tValidation Loss: 0.896197 \tAccuracy: 0.532967\n",
      "Epoch: 53 \tTraining Loss: 0.746939 \tValidation Loss: 0.896188 \tAccuracy: 0.543956\n",
      "Epoch: 54 \tTraining Loss: 0.744513 \tValidation Loss: 0.895792 \tAccuracy: 0.543956\n",
      "Epoch: 55 \tTraining Loss: 0.742195 \tValidation Loss: 0.897516 \tAccuracy: 0.549451\n",
      "Epoch: 56 \tTraining Loss: 0.740187 \tValidation Loss: 0.898280 \tAccuracy: 0.543956\n",
      "Epoch: 57 \tTraining Loss: 0.737607 \tValidation Loss: 0.898455 \tAccuracy: 0.532967\n",
      "Epoch: 58 \tTraining Loss: 0.735502 \tValidation Loss: 0.899846 \tAccuracy: 0.532967\n",
      "Epoch: 59 \tTraining Loss: 0.733378 \tValidation Loss: 0.900142 \tAccuracy: 0.527473\n",
      "Epoch: 60 \tTraining Loss: 0.731301 \tValidation Loss: 0.900659 \tAccuracy: 0.527473\n",
      "Epoch: 61 \tTraining Loss: 0.728908 \tValidation Loss: 0.901611 \tAccuracy: 0.527473\n",
      "Epoch: 62 \tTraining Loss: 0.726577 \tValidation Loss: 0.901385 \tAccuracy: 0.527473\n",
      "Epoch: 63 \tTraining Loss: 0.724160 \tValidation Loss: 0.902040 \tAccuracy: 0.532967\n",
      "Epoch: 64 \tTraining Loss: 0.722249 \tValidation Loss: 0.901726 \tAccuracy: 0.527473\n",
      "Epoch: 65 \tTraining Loss: 0.719509 \tValidation Loss: 0.903116 \tAccuracy: 0.527473\n",
      "Epoch: 66 \tTraining Loss: 0.717123 \tValidation Loss: 0.904019 \tAccuracy: 0.532967\n",
      "Epoch: 67 \tTraining Loss: 0.714787 \tValidation Loss: 0.904235 \tAccuracy: 0.538462\n",
      "Epoch: 68 \tTraining Loss: 0.712114 \tValidation Loss: 0.904376 \tAccuracy: 0.538462\n",
      "Epoch: 69 \tTraining Loss: 0.709636 \tValidation Loss: 0.905747 \tAccuracy: 0.543956\n",
      "Epoch: 70 \tTraining Loss: 0.707354 \tValidation Loss: 0.905485 \tAccuracy: 0.543956\n",
      "Epoch: 71 \tTraining Loss: 0.704790 \tValidation Loss: 0.905106 \tAccuracy: 0.543956\n",
      "Epoch: 72 \tTraining Loss: 0.702237 \tValidation Loss: 0.905700 \tAccuracy: 0.549451\n",
      "Epoch: 73 \tTraining Loss: 0.699866 \tValidation Loss: 0.905726 \tAccuracy: 0.549451\n",
      "Epoch: 74 \tTraining Loss: 0.697328 \tValidation Loss: 0.905331 \tAccuracy: 0.549451\n",
      "Epoch: 75 \tTraining Loss: 0.695027 \tValidation Loss: 0.906005 \tAccuracy: 0.549451\n",
      "Epoch: 76 \tTraining Loss: 0.692776 \tValidation Loss: 0.905192 \tAccuracy: 0.549451\n",
      "Epoch: 77 \tTraining Loss: 0.690396 \tValidation Loss: 0.906125 \tAccuracy: 0.554945\n",
      "Epoch: 78 \tTraining Loss: 0.688305 \tValidation Loss: 0.904779 \tAccuracy: 0.560440\n",
      "Epoch: 79 \tTraining Loss: 0.686258 \tValidation Loss: 0.904975 \tAccuracy: 0.560440\n",
      "Epoch: 80 \tTraining Loss: 0.684109 \tValidation Loss: 0.904784 \tAccuracy: 0.560440\n",
      "Epoch: 81 \tTraining Loss: 0.682269 \tValidation Loss: 0.904791 \tAccuracy: 0.565934\n",
      "Epoch: 82 \tTraining Loss: 0.680490 \tValidation Loss: 0.904390 \tAccuracy: 0.560440\n",
      "Epoch: 83 \tTraining Loss: 0.678727 \tValidation Loss: 0.904821 \tAccuracy: 0.554945\n",
      "Epoch: 84 \tTraining Loss: 0.677185 \tValidation Loss: 0.904481 \tAccuracy: 0.554945\n",
      "Epoch: 85 \tTraining Loss: 0.675739 \tValidation Loss: 0.904258 \tAccuracy: 0.549451\n",
      "Epoch: 86 \tTraining Loss: 0.674277 \tValidation Loss: 0.904151 \tAccuracy: 0.554945\n",
      "Epoch: 87 \tTraining Loss: 0.673026 \tValidation Loss: 0.904157 \tAccuracy: 0.554945\n",
      "Epoch: 88 \tTraining Loss: 0.671812 \tValidation Loss: 0.904068 \tAccuracy: 0.554945\n",
      "Epoch: 89 \tTraining Loss: 0.670721 \tValidation Loss: 0.904137 \tAccuracy: 0.554945\n",
      "Epoch: 90 \tTraining Loss: 0.669835 \tValidation Loss: 0.904029 \tAccuracy: 0.554945\n",
      "Epoch: 91 \tTraining Loss: 0.668975 \tValidation Loss: 0.904086 \tAccuracy: 0.554945\n",
      "Epoch: 92 \tTraining Loss: 0.668334 \tValidation Loss: 0.904200 \tAccuracy: 0.554945\n",
      "Epoch: 93 \tTraining Loss: 0.667760 \tValidation Loss: 0.904062 \tAccuracy: 0.554945\n",
      "Epoch: 94 \tTraining Loss: 0.667269 \tValidation Loss: 0.904146 \tAccuracy: 0.554945\n",
      "Epoch: 95 \tTraining Loss: 0.666914 \tValidation Loss: 0.904238 \tAccuracy: 0.554945\n",
      "Epoch: 96 \tTraining Loss: 0.666651 \tValidation Loss: 0.904414 \tAccuracy: 0.554945\n",
      "Epoch: 97 \tTraining Loss: 0.666507 \tValidation Loss: 0.904509 \tAccuracy: 0.554945\n",
      "Epoch: 98 \tTraining Loss: 0.666385 \tValidation Loss: 0.904733 \tAccuracy: 0.554945\n",
      "Epoch: 99 \tTraining Loss: 0.666385 \tValidation Loss: 0.905006 \tAccuracy: 0.554945\n",
      "Epoch: 100 \tTraining Loss: 0.666409 \tValidation Loss: 0.905299 \tAccuracy: 0.554945\n",
      "Epoch: 101 \tTraining Loss: 0.666460 \tValidation Loss: 0.905684 \tAccuracy: 0.554945\n",
      "Epoch: 102 \tTraining Loss: 0.666602 \tValidation Loss: 0.906248 \tAccuracy: 0.554945\n",
      "Epoch: 103 \tTraining Loss: 0.666782 \tValidation Loss: 0.906711 \tAccuracy: 0.554945\n",
      "Epoch: 104 \tTraining Loss: 0.666890 \tValidation Loss: 0.907471 \tAccuracy: 0.554945\n",
      "Epoch: 105 \tTraining Loss: 0.667041 \tValidation Loss: 0.908067 \tAccuracy: 0.554945\n",
      "Epoch: 106 \tTraining Loss: 0.667187 \tValidation Loss: 0.909054 \tAccuracy: 0.554945\n",
      "Epoch: 107 \tTraining Loss: 0.667432 \tValidation Loss: 0.909662 \tAccuracy: 0.554945\n",
      "Epoch: 108 \tTraining Loss: 0.667353 \tValidation Loss: 0.910817 \tAccuracy: 0.560440\n",
      "Epoch: 109 \tTraining Loss: 0.667583 \tValidation Loss: 0.911344 \tAccuracy: 0.560440\n",
      "Epoch: 110 \tTraining Loss: 0.667599 \tValidation Loss: 0.912827 \tAccuracy: 0.560440\n",
      "Epoch: 111 \tTraining Loss: 0.667601 \tValidation Loss: 0.913490 \tAccuracy: 0.560440\n",
      "Epoch: 112 \tTraining Loss: 0.667491 \tValidation Loss: 0.914324 \tAccuracy: 0.565934\n",
      "Epoch: 113 \tTraining Loss: 0.667297 \tValidation Loss: 0.915086 \tAccuracy: 0.565934\n",
      "Epoch: 114 \tTraining Loss: 0.667168 \tValidation Loss: 0.915946 \tAccuracy: 0.565934\n",
      "Epoch: 115 \tTraining Loss: 0.666833 \tValidation Loss: 0.915868 \tAccuracy: 0.565934\n",
      "Epoch: 116 \tTraining Loss: 0.666295 \tValidation Loss: 0.917383 \tAccuracy: 0.565934\n",
      "Epoch: 117 \tTraining Loss: 0.666098 \tValidation Loss: 0.918378 \tAccuracy: 0.571429\n",
      "Epoch: 118 \tTraining Loss: 0.665609 \tValidation Loss: 0.919537 \tAccuracy: 0.571429\n",
      "Epoch: 119 \tTraining Loss: 0.665074 \tValidation Loss: 0.920032 \tAccuracy: 0.565934\n",
      "Epoch: 120 \tTraining Loss: 0.663997 \tValidation Loss: 0.921592 \tAccuracy: 0.571429\n",
      "Epoch: 121 \tTraining Loss: 0.663324 \tValidation Loss: 0.920874 \tAccuracy: 0.565934\n",
      "Epoch: 122 \tTraining Loss: 0.662206 \tValidation Loss: 0.922433 \tAccuracy: 0.571429\n",
      "Epoch: 123 \tTraining Loss: 0.661539 \tValidation Loss: 0.923348 \tAccuracy: 0.571429\n",
      "Epoch: 124 \tTraining Loss: 0.660469 \tValidation Loss: 0.924568 \tAccuracy: 0.571429\n",
      "Epoch: 125 \tTraining Loss: 0.659396 \tValidation Loss: 0.923795 \tAccuracy: 0.571429\n",
      "Epoch: 126 \tTraining Loss: 0.658662 \tValidation Loss: 0.925216 \tAccuracy: 0.571429\n",
      "Epoch: 127 \tTraining Loss: 0.657148 \tValidation Loss: 0.926602 \tAccuracy: 0.571429\n",
      "Epoch: 128 \tTraining Loss: 0.655472 \tValidation Loss: 0.927564 \tAccuracy: 0.560440\n",
      "Epoch: 129 \tTraining Loss: 0.654311 \tValidation Loss: 0.928159 \tAccuracy: 0.565934\n",
      "Epoch: 130 \tTraining Loss: 0.652608 \tValidation Loss: 0.930312 \tAccuracy: 0.571429\n",
      "Epoch: 131 \tTraining Loss: 0.650881 \tValidation Loss: 0.931844 \tAccuracy: 0.571429\n",
      "Epoch: 132 \tTraining Loss: 0.649206 \tValidation Loss: 0.934009 \tAccuracy: 0.576923\n",
      "Epoch: 133 \tTraining Loss: 0.647294 \tValidation Loss: 0.933263 \tAccuracy: 0.571429\n",
      "Epoch: 134 \tTraining Loss: 0.645223 \tValidation Loss: 0.934035 \tAccuracy: 0.571429\n",
      "Epoch: 135 \tTraining Loss: 0.643262 \tValidation Loss: 0.934784 \tAccuracy: 0.571429\n",
      "Epoch: 136 \tTraining Loss: 0.641513 \tValidation Loss: 0.935079 \tAccuracy: 0.571429\n",
      "Epoch: 137 \tTraining Loss: 0.639362 \tValidation Loss: 0.935478 \tAccuracy: 0.565934\n",
      "Epoch: 138 \tTraining Loss: 0.637501 \tValidation Loss: 0.936733 \tAccuracy: 0.571429\n",
      "Epoch: 139 \tTraining Loss: 0.635800 \tValidation Loss: 0.936214 \tAccuracy: 0.571429\n",
      "Epoch: 140 \tTraining Loss: 0.633492 \tValidation Loss: 0.936693 \tAccuracy: 0.571429\n",
      "Epoch: 141 \tTraining Loss: 0.631350 \tValidation Loss: 0.935849 \tAccuracy: 0.571429\n",
      "Epoch: 142 \tTraining Loss: 0.629770 \tValidation Loss: 0.935716 \tAccuracy: 0.571429\n",
      "Epoch: 143 \tTraining Loss: 0.627500 \tValidation Loss: 0.935886 \tAccuracy: 0.571429\n",
      "Epoch: 144 \tTraining Loss: 0.625950 \tValidation Loss: 0.935718 \tAccuracy: 0.571429\n",
      "Epoch: 145 \tTraining Loss: 0.623577 \tValidation Loss: 0.935778 \tAccuracy: 0.571429\n",
      "Epoch: 146 \tTraining Loss: 0.622101 \tValidation Loss: 0.935735 \tAccuracy: 0.571429\n",
      "Epoch: 147 \tTraining Loss: 0.620024 \tValidation Loss: 0.934025 \tAccuracy: 0.571429\n",
      "Epoch: 148 \tTraining Loss: 0.618713 \tValidation Loss: 0.934707 \tAccuracy: 0.560440\n",
      "Epoch: 149 \tTraining Loss: 0.616887 \tValidation Loss: 0.933837 \tAccuracy: 0.565934\n",
      "Epoch: 150 \tTraining Loss: 0.615614 \tValidation Loss: 0.933435 \tAccuracy: 0.571429\n",
      "Epoch: 151 \tTraining Loss: 0.614199 \tValidation Loss: 0.933556 \tAccuracy: 0.565934\n",
      "Epoch: 152 \tTraining Loss: 0.613008 \tValidation Loss: 0.933354 \tAccuracy: 0.565934\n",
      "Epoch: 153 \tTraining Loss: 0.611687 \tValidation Loss: 0.933000 \tAccuracy: 0.565934\n",
      "Epoch: 154 \tTraining Loss: 0.610762 \tValidation Loss: 0.933047 \tAccuracy: 0.565934\n",
      "Epoch: 155 \tTraining Loss: 0.609893 \tValidation Loss: 0.933110 \tAccuracy: 0.565934\n",
      "Epoch: 156 \tTraining Loss: 0.609144 \tValidation Loss: 0.932983 \tAccuracy: 0.565934\n",
      "Epoch: 157 \tTraining Loss: 0.608520 \tValidation Loss: 0.933073 \tAccuracy: 0.560440\n",
      "Epoch: 158 \tTraining Loss: 0.608057 \tValidation Loss: 0.933061 \tAccuracy: 0.560440\n",
      "Epoch: 159 \tTraining Loss: 0.607632 \tValidation Loss: 0.933082 \tAccuracy: 0.565934\n",
      "Epoch: 160 \tTraining Loss: 0.607362 \tValidation Loss: 0.933184 \tAccuracy: 0.565934\n",
      "Epoch: 161 \tTraining Loss: 0.607159 \tValidation Loss: 0.933384 \tAccuracy: 0.565934\n",
      "Epoch: 162 \tTraining Loss: 0.607120 \tValidation Loss: 0.933585 \tAccuracy: 0.571429\n",
      "Epoch: 163 \tTraining Loss: 0.607120 \tValidation Loss: 0.933925 \tAccuracy: 0.565934\n",
      "Epoch: 164 \tTraining Loss: 0.607165 \tValidation Loss: 0.934117 \tAccuracy: 0.571429\n",
      "Epoch: 165 \tTraining Loss: 0.607424 \tValidation Loss: 0.934581 \tAccuracy: 0.571429\n",
      "Epoch: 166 \tTraining Loss: 0.607520 \tValidation Loss: 0.935421 \tAccuracy: 0.576923\n",
      "Epoch: 167 \tTraining Loss: 0.607813 \tValidation Loss: 0.935682 \tAccuracy: 0.571429\n",
      "Epoch: 168 \tTraining Loss: 0.607984 \tValidation Loss: 0.937055 \tAccuracy: 0.576923\n",
      "Epoch: 169 \tTraining Loss: 0.608267 \tValidation Loss: 0.937752 \tAccuracy: 0.576923\n",
      "Epoch: 170 \tTraining Loss: 0.608691 \tValidation Loss: 0.938932 \tAccuracy: 0.571429\n",
      "Epoch: 171 \tTraining Loss: 0.608882 \tValidation Loss: 0.940294 \tAccuracy: 0.571429\n",
      "Epoch: 172 \tTraining Loss: 0.609307 \tValidation Loss: 0.941095 \tAccuracy: 0.576923\n",
      "Epoch: 173 \tTraining Loss: 0.609219 \tValidation Loss: 0.943412 \tAccuracy: 0.576923\n",
      "Epoch: 174 \tTraining Loss: 0.609687 \tValidation Loss: 0.944329 \tAccuracy: 0.576923\n",
      "Epoch: 175 \tTraining Loss: 0.609725 \tValidation Loss: 0.945840 \tAccuracy: 0.576923\n",
      "Epoch: 176 \tTraining Loss: 0.610064 \tValidation Loss: 0.947088 \tAccuracy: 0.576923\n",
      "Epoch: 177 \tTraining Loss: 0.609387 \tValidation Loss: 0.949101 \tAccuracy: 0.576923\n",
      "Epoch: 178 \tTraining Loss: 0.609570 \tValidation Loss: 0.950483 \tAccuracy: 0.582418\n",
      "Epoch: 179 \tTraining Loss: 0.609302 \tValidation Loss: 0.952076 \tAccuracy: 0.587912\n",
      "Epoch: 180 \tTraining Loss: 0.608953 \tValidation Loss: 0.954575 \tAccuracy: 0.582418\n",
      "Epoch: 181 \tTraining Loss: 0.608541 \tValidation Loss: 0.956847 \tAccuracy: 0.576923\n",
      "Epoch: 182 \tTraining Loss: 0.607829 \tValidation Loss: 0.957340 \tAccuracy: 0.582418\n",
      "Epoch: 183 \tTraining Loss: 0.607549 \tValidation Loss: 0.960998 \tAccuracy: 0.576923\n",
      "Epoch: 184 \tTraining Loss: 0.606402 \tValidation Loss: 0.963009 \tAccuracy: 0.582418\n",
      "Epoch: 185 \tTraining Loss: 0.605741 \tValidation Loss: 0.964604 \tAccuracy: 0.582418\n",
      "Epoch: 186 \tTraining Loss: 0.605143 \tValidation Loss: 0.965879 \tAccuracy: 0.576923\n",
      "Epoch: 187 \tTraining Loss: 0.603493 \tValidation Loss: 0.968433 \tAccuracy: 0.582418\n",
      "Epoch: 188 \tTraining Loss: 0.602890 \tValidation Loss: 0.968746 \tAccuracy: 0.571429\n",
      "Epoch: 189 \tTraining Loss: 0.601331 \tValidation Loss: 0.970923 \tAccuracy: 0.571429\n",
      "Epoch: 190 \tTraining Loss: 0.599862 \tValidation Loss: 0.975169 \tAccuracy: 0.571429\n",
      "Epoch: 191 \tTraining Loss: 0.598741 \tValidation Loss: 0.974538 \tAccuracy: 0.571429\n",
      "Epoch: 192 \tTraining Loss: 0.595886 \tValidation Loss: 0.977339 \tAccuracy: 0.576923\n",
      "Epoch: 193 \tTraining Loss: 0.594830 \tValidation Loss: 0.979922 \tAccuracy: 0.565934\n",
      "Epoch: 194 \tTraining Loss: 0.592624 \tValidation Loss: 0.983618 \tAccuracy: 0.571429\n",
      "Epoch: 195 \tTraining Loss: 0.590859 \tValidation Loss: 0.984988 \tAccuracy: 0.571429\n",
      "Epoch: 196 \tTraining Loss: 0.589552 \tValidation Loss: 0.984883 \tAccuracy: 0.576923\n",
      "Epoch: 197 \tTraining Loss: 0.587017 \tValidation Loss: 0.987143 \tAccuracy: 0.576923\n",
      "Epoch: 198 \tTraining Loss: 0.584732 \tValidation Loss: 0.986123 \tAccuracy: 0.571429\n",
      "Epoch: 199 \tTraining Loss: 0.582681 \tValidation Loss: 0.987762 \tAccuracy: 0.571429\n",
      "Epoch: 200 \tTraining Loss: 0.579840 \tValidation Loss: 0.989525 \tAccuracy: 0.571429\n",
      "Epoch: 201 \tTraining Loss: 0.578377 \tValidation Loss: 0.990594 \tAccuracy: 0.576923\n",
      "Epoch: 202 \tTraining Loss: 0.576627 \tValidation Loss: 0.991220 \tAccuracy: 0.571429\n",
      "Epoch: 203 \tTraining Loss: 0.574129 \tValidation Loss: 0.991061 \tAccuracy: 0.571429\n",
      "Epoch: 204 \tTraining Loss: 0.572264 \tValidation Loss: 0.992361 \tAccuracy: 0.571429\n",
      "Epoch: 205 \tTraining Loss: 0.569975 \tValidation Loss: 0.993071 \tAccuracy: 0.571429\n",
      "Epoch: 206 \tTraining Loss: 0.567853 \tValidation Loss: 0.992414 \tAccuracy: 0.571429\n",
      "Epoch: 207 \tTraining Loss: 0.565755 \tValidation Loss: 0.993789 \tAccuracy: 0.571429\n",
      "Epoch: 208 \tTraining Loss: 0.563507 \tValidation Loss: 0.993583 \tAccuracy: 0.565934\n",
      "Epoch: 209 \tTraining Loss: 0.561376 \tValidation Loss: 0.993130 \tAccuracy: 0.571429\n",
      "Epoch: 210 \tTraining Loss: 0.559853 \tValidation Loss: 0.994078 \tAccuracy: 0.571429\n",
      "Epoch: 211 \tTraining Loss: 0.557587 \tValidation Loss: 0.995393 \tAccuracy: 0.565934\n",
      "Epoch: 212 \tTraining Loss: 0.555942 \tValidation Loss: 0.994363 \tAccuracy: 0.565934\n",
      "Epoch: 213 \tTraining Loss: 0.554236 \tValidation Loss: 0.995528 \tAccuracy: 0.565934\n",
      "Epoch: 214 \tTraining Loss: 0.552590 \tValidation Loss: 0.995333 \tAccuracy: 0.565934\n",
      "Epoch: 215 \tTraining Loss: 0.551083 \tValidation Loss: 0.995338 \tAccuracy: 0.565934\n",
      "Epoch: 216 \tTraining Loss: 0.549719 \tValidation Loss: 0.995533 \tAccuracy: 0.560440\n",
      "Epoch: 217 \tTraining Loss: 0.548316 \tValidation Loss: 0.995754 \tAccuracy: 0.560440\n",
      "Epoch: 218 \tTraining Loss: 0.547266 \tValidation Loss: 0.996020 \tAccuracy: 0.565934\n",
      "Epoch: 219 \tTraining Loss: 0.546086 \tValidation Loss: 0.996848 \tAccuracy: 0.565934\n",
      "Epoch: 220 \tTraining Loss: 0.545387 \tValidation Loss: 0.996190 \tAccuracy: 0.565934\n",
      "Epoch: 221 \tTraining Loss: 0.544596 \tValidation Loss: 0.996582 \tAccuracy: 0.576923\n",
      "Epoch: 222 \tTraining Loss: 0.544047 \tValidation Loss: 0.996663 \tAccuracy: 0.571429\n",
      "Epoch: 223 \tTraining Loss: 0.543590 \tValidation Loss: 0.997196 \tAccuracy: 0.571429\n",
      "Epoch: 224 \tTraining Loss: 0.543359 \tValidation Loss: 0.997086 \tAccuracy: 0.576923\n",
      "Epoch: 225 \tTraining Loss: 0.543120 \tValidation Loss: 0.997477 \tAccuracy: 0.571429\n",
      "Epoch: 226 \tTraining Loss: 0.543164 \tValidation Loss: 0.997961 \tAccuracy: 0.571429\n",
      "Epoch: 227 \tTraining Loss: 0.543055 \tValidation Loss: 0.998333 \tAccuracy: 0.571429\n",
      "Epoch: 228 \tTraining Loss: 0.543302 \tValidation Loss: 0.998588 \tAccuracy: 0.571429\n",
      "Epoch: 229 \tTraining Loss: 0.543600 \tValidation Loss: 0.999547 \tAccuracy: 0.576923\n",
      "Epoch: 230 \tTraining Loss: 0.543673 \tValidation Loss: 1.000694 \tAccuracy: 0.565934\n",
      "Epoch: 231 \tTraining Loss: 0.544225 \tValidation Loss: 1.001494 \tAccuracy: 0.565934\n",
      "Epoch: 232 \tTraining Loss: 0.544475 \tValidation Loss: 1.001979 \tAccuracy: 0.565934\n",
      "Epoch: 233 \tTraining Loss: 0.544923 \tValidation Loss: 1.003051 \tAccuracy: 0.565934\n",
      "Epoch: 234 \tTraining Loss: 0.545151 \tValidation Loss: 1.005033 \tAccuracy: 0.565934\n",
      "Epoch: 235 \tTraining Loss: 0.546228 \tValidation Loss: 1.005501 \tAccuracy: 0.571429\n",
      "Epoch: 236 \tTraining Loss: 0.546010 \tValidation Loss: 1.006852 \tAccuracy: 0.576923\n",
      "Epoch: 237 \tTraining Loss: 0.546505 \tValidation Loss: 1.008874 \tAccuracy: 0.576923\n",
      "Epoch: 238 \tTraining Loss: 0.546607 \tValidation Loss: 1.010502 \tAccuracy: 0.571429\n",
      "Epoch: 239 \tTraining Loss: 0.547445 \tValidation Loss: 1.010212 \tAccuracy: 0.571429\n",
      "Epoch: 240 \tTraining Loss: 0.547590 \tValidation Loss: 1.012396 \tAccuracy: 0.565934\n",
      "Epoch: 241 \tTraining Loss: 0.547803 \tValidation Loss: 1.014696 \tAccuracy: 0.571429\n",
      "Epoch: 242 \tTraining Loss: 0.547167 \tValidation Loss: 1.015955 \tAccuracy: 0.571429\n",
      "Epoch: 243 \tTraining Loss: 0.547500 \tValidation Loss: 1.019467 \tAccuracy: 0.571429\n",
      "Epoch: 244 \tTraining Loss: 0.547629 \tValidation Loss: 1.020926 \tAccuracy: 0.560440\n",
      "Epoch: 245 \tTraining Loss: 0.547271 \tValidation Loss: 1.022678 \tAccuracy: 0.576923\n",
      "Epoch: 246 \tTraining Loss: 0.546327 \tValidation Loss: 1.024419 \tAccuracy: 0.571429\n",
      "Epoch: 247 \tTraining Loss: 0.545768 \tValidation Loss: 1.028330 \tAccuracy: 0.576923\n",
      "Epoch: 248 \tTraining Loss: 0.546026 \tValidation Loss: 1.029341 \tAccuracy: 0.582418\n",
      "Epoch: 249 \tTraining Loss: 0.544436 \tValidation Loss: 1.031703 \tAccuracy: 0.576923\n",
      "Epoch: 0 \tTraining Loss: 1.096209 \tValidation Loss: 1.098313 \tAccuracy: 0.335165\n",
      "Better loss found. (inf --> 1.098313).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.054993 \tValidation Loss: 1.059162 \tAccuracy: 0.412088\n",
      "Better loss found. (1.098313 --> 1.059162).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.009312 \tValidation Loss: 1.022164 \tAccuracy: 0.445055\n",
      "Better loss found. (1.059162 --> 1.022164).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.971136 \tValidation Loss: 0.993297 \tAccuracy: 0.505495\n",
      "Better loss found. (1.022164 --> 0.993297).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.939077 \tValidation Loss: 0.972517 \tAccuracy: 0.521978\n",
      "Better loss found. (0.993297 --> 0.972517).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.913712 \tValidation Loss: 0.957905 \tAccuracy: 0.532967\n",
      "Better loss found. (0.972517 --> 0.957905).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.893746 \tValidation Loss: 0.947223 \tAccuracy: 0.532967\n",
      "Better loss found. (0.957905 --> 0.947223).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.878380 \tValidation Loss: 0.939448 \tAccuracy: 0.527473\n",
      "Better loss found. (0.947223 --> 0.939448).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.866574 \tValidation Loss: 0.932727 \tAccuracy: 0.543956\n",
      "Better loss found. (0.939448 --> 0.932727).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.856605 \tValidation Loss: 0.926736 \tAccuracy: 0.560440\n",
      "Better loss found. (0.932727 --> 0.926736).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.847699 \tValidation Loss: 0.922080 \tAccuracy: 0.560440\n",
      "Better loss found. (0.926736 --> 0.922080).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.839933 \tValidation Loss: 0.917948 \tAccuracy: 0.565934\n",
      "Better loss found. (0.922080 --> 0.917948).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.832672 \tValidation Loss: 0.913209 \tAccuracy: 0.565934\n",
      "Better loss found. (0.917948 --> 0.913209).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.826037 \tValidation Loss: 0.908272 \tAccuracy: 0.571429\n",
      "Better loss found. (0.913209 --> 0.908272).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.819848 \tValidation Loss: 0.905983 \tAccuracy: 0.560440\n",
      "Better loss found. (0.908272 --> 0.905983).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.814460 \tValidation Loss: 0.903551 \tAccuracy: 0.554945\n",
      "Better loss found. (0.905983 --> 0.903551).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.809125 \tValidation Loss: 0.899644 \tAccuracy: 0.560440\n",
      "Better loss found. (0.903551 --> 0.899644).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.803220 \tValidation Loss: 0.896800 \tAccuracy: 0.554945\n",
      "Better loss found. (0.899644 --> 0.896800).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.798423 \tValidation Loss: 0.894191 \tAccuracy: 0.565934\n",
      "Better loss found. (0.896800 --> 0.894191).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.793808 \tValidation Loss: 0.893315 \tAccuracy: 0.565934\n",
      "Better loss found. (0.894191 --> 0.893315).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.789906 \tValidation Loss: 0.891970 \tAccuracy: 0.565934\n",
      "Better loss found. (0.893315 --> 0.891970).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.786520 \tValidation Loss: 0.890813 \tAccuracy: 0.560440\n",
      "Better loss found. (0.891970 --> 0.890813).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.783613 \tValidation Loss: 0.889748 \tAccuracy: 0.560440\n",
      "Better loss found. (0.890813 --> 0.889748).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.781168 \tValidation Loss: 0.888910 \tAccuracy: 0.560440\n",
      "Better loss found. (0.889748 --> 0.888910).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.778930 \tValidation Loss: 0.888247 \tAccuracy: 0.565934\n",
      "Better loss found. (0.888910 --> 0.888247).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.777002 \tValidation Loss: 0.887761 \tAccuracy: 0.565934\n",
      "Better loss found. (0.888247 --> 0.887761).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.775405 \tValidation Loss: 0.887250 \tAccuracy: 0.560440\n",
      "Better loss found. (0.887761 --> 0.887250).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.773968 \tValidation Loss: 0.886726 \tAccuracy: 0.560440\n",
      "Better loss found. (0.887250 --> 0.886726).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.772753 \tValidation Loss: 0.886434 \tAccuracy: 0.560440\n",
      "Better loss found. (0.886726 --> 0.886434).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.771729 \tValidation Loss: 0.886214 \tAccuracy: 0.560440\n",
      "Better loss found. (0.886434 --> 0.886214).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.770893 \tValidation Loss: 0.886031 \tAccuracy: 0.560440\n",
      "Better loss found. (0.886214 --> 0.886031).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.770184 \tValidation Loss: 0.885843 \tAccuracy: 0.560440\n",
      "Better loss found. (0.886031 --> 0.885843).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.769508 \tValidation Loss: 0.885724 \tAccuracy: 0.560440\n",
      "Better loss found. (0.885843 --> 0.885724).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.768995 \tValidation Loss: 0.885577 \tAccuracy: 0.560440\n",
      "Better loss found. (0.885724 --> 0.885577).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.768479 \tValidation Loss: 0.885479 \tAccuracy: 0.560440\n",
      "Better loss found. (0.885577 --> 0.885479).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.768039 \tValidation Loss: 0.885352 \tAccuracy: 0.560440\n",
      "Better loss found. (0.885479 --> 0.885352).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.767595 \tValidation Loss: 0.885372 \tAccuracy: 0.560440\n",
      "Epoch: 37 \tTraining Loss: 0.767114 \tValidation Loss: 0.885256 \tAccuracy: 0.560440\n",
      "Better loss found. (0.885352 --> 0.885256).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.766639 \tValidation Loss: 0.885332 \tAccuracy: 0.560440\n",
      "Epoch: 39 \tTraining Loss: 0.766089 \tValidation Loss: 0.885323 \tAccuracy: 0.554945\n",
      "Epoch: 40 \tTraining Loss: 0.765436 \tValidation Loss: 0.885421 \tAccuracy: 0.554945\n",
      "Epoch: 41 \tTraining Loss: 0.764804 \tValidation Loss: 0.885539 \tAccuracy: 0.554945\n",
      "Epoch: 42 \tTraining Loss: 0.763961 \tValidation Loss: 0.885871 \tAccuracy: 0.560440\n",
      "Epoch: 43 \tTraining Loss: 0.762996 \tValidation Loss: 0.885955 \tAccuracy: 0.565934\n",
      "Epoch: 44 \tTraining Loss: 0.761978 \tValidation Loss: 0.886096 \tAccuracy: 0.565934\n",
      "Epoch: 45 \tTraining Loss: 0.760715 \tValidation Loss: 0.886624 \tAccuracy: 0.571429\n",
      "Epoch: 46 \tTraining Loss: 0.759449 \tValidation Loss: 0.887152 \tAccuracy: 0.571429\n",
      "Epoch: 47 \tTraining Loss: 0.757916 \tValidation Loss: 0.887621 \tAccuracy: 0.582418\n",
      "Epoch: 48 \tTraining Loss: 0.756427 \tValidation Loss: 0.888336 \tAccuracy: 0.582418\n",
      "Epoch: 49 \tTraining Loss: 0.754835 \tValidation Loss: 0.888911 \tAccuracy: 0.582418\n",
      "Epoch: 50 \tTraining Loss: 0.753064 \tValidation Loss: 0.889173 \tAccuracy: 0.587912\n",
      "Epoch: 51 \tTraining Loss: 0.750977 \tValidation Loss: 0.890348 \tAccuracy: 0.593407\n",
      "Epoch: 52 \tTraining Loss: 0.748917 \tValidation Loss: 0.890627 \tAccuracy: 0.598901\n",
      "Epoch: 53 \tTraining Loss: 0.746430 \tValidation Loss: 0.891790 \tAccuracy: 0.598901\n",
      "Epoch: 54 \tTraining Loss: 0.744286 \tValidation Loss: 0.893043 \tAccuracy: 0.587912\n",
      "Epoch: 55 \tTraining Loss: 0.742046 \tValidation Loss: 0.894381 \tAccuracy: 0.582418\n",
      "Epoch: 56 \tTraining Loss: 0.739823 \tValidation Loss: 0.895334 \tAccuracy: 0.587912\n",
      "Epoch: 57 \tTraining Loss: 0.737583 \tValidation Loss: 0.895738 \tAccuracy: 0.587912\n",
      "Epoch: 58 \tTraining Loss: 0.735071 \tValidation Loss: 0.896406 \tAccuracy: 0.587912\n",
      "Epoch: 59 \tTraining Loss: 0.732723 \tValidation Loss: 0.897445 \tAccuracy: 0.582418\n",
      "Epoch: 60 \tTraining Loss: 0.730263 \tValidation Loss: 0.898574 \tAccuracy: 0.582418\n",
      "Epoch: 61 \tTraining Loss: 0.727599 \tValidation Loss: 0.899732 \tAccuracy: 0.576923\n",
      "Epoch: 62 \tTraining Loss: 0.725551 \tValidation Loss: 0.900929 \tAccuracy: 0.582418\n",
      "Epoch: 63 \tTraining Loss: 0.722674 \tValidation Loss: 0.902244 \tAccuracy: 0.576923\n",
      "Epoch: 64 \tTraining Loss: 0.720146 \tValidation Loss: 0.903717 \tAccuracy: 0.576923\n",
      "Epoch: 65 \tTraining Loss: 0.717613 \tValidation Loss: 0.904141 \tAccuracy: 0.576923\n",
      "Epoch: 66 \tTraining Loss: 0.714601 \tValidation Loss: 0.905899 \tAccuracy: 0.576923\n",
      "Epoch: 67 \tTraining Loss: 0.711899 \tValidation Loss: 0.907303 \tAccuracy: 0.576923\n",
      "Epoch: 68 \tTraining Loss: 0.709323 \tValidation Loss: 0.908079 \tAccuracy: 0.576923\n",
      "Epoch: 69 \tTraining Loss: 0.706657 \tValidation Loss: 0.909200 \tAccuracy: 0.582418\n",
      "Epoch: 70 \tTraining Loss: 0.703621 \tValidation Loss: 0.910504 \tAccuracy: 0.582418\n",
      "Epoch: 71 \tTraining Loss: 0.701118 \tValidation Loss: 0.911093 \tAccuracy: 0.582418\n",
      "Epoch: 72 \tTraining Loss: 0.698524 \tValidation Loss: 0.911763 \tAccuracy: 0.576923\n",
      "Epoch: 73 \tTraining Loss: 0.695668 \tValidation Loss: 0.913391 \tAccuracy: 0.576923\n",
      "Epoch: 74 \tTraining Loss: 0.693117 \tValidation Loss: 0.915245 \tAccuracy: 0.571429\n",
      "Epoch: 75 \tTraining Loss: 0.690534 \tValidation Loss: 0.916112 \tAccuracy: 0.576923\n",
      "Epoch: 76 \tTraining Loss: 0.688026 \tValidation Loss: 0.917506 \tAccuracy: 0.576923\n",
      "Epoch: 77 \tTraining Loss: 0.685705 \tValidation Loss: 0.918950 \tAccuracy: 0.571429\n",
      "Epoch: 78 \tTraining Loss: 0.683363 \tValidation Loss: 0.919461 \tAccuracy: 0.576923\n",
      "Epoch: 79 \tTraining Loss: 0.680933 \tValidation Loss: 0.920347 \tAccuracy: 0.582418\n",
      "Epoch: 80 \tTraining Loss: 0.678765 \tValidation Loss: 0.920678 \tAccuracy: 0.587912\n",
      "Epoch: 81 \tTraining Loss: 0.676682 \tValidation Loss: 0.921214 \tAccuracy: 0.587912\n",
      "Epoch: 82 \tTraining Loss: 0.674574 \tValidation Loss: 0.921361 \tAccuracy: 0.587912\n",
      "Epoch: 83 \tTraining Loss: 0.672501 \tValidation Loss: 0.921821 \tAccuracy: 0.587912\n",
      "Epoch: 84 \tTraining Loss: 0.670599 \tValidation Loss: 0.921939 \tAccuracy: 0.587912\n",
      "Epoch: 85 \tTraining Loss: 0.668835 \tValidation Loss: 0.922093 \tAccuracy: 0.587912\n",
      "Epoch: 86 \tTraining Loss: 0.667082 \tValidation Loss: 0.922066 \tAccuracy: 0.587912\n",
      "Epoch: 87 \tTraining Loss: 0.665590 \tValidation Loss: 0.922471 \tAccuracy: 0.587912\n",
      "Epoch: 88 \tTraining Loss: 0.664148 \tValidation Loss: 0.922430 \tAccuracy: 0.582418\n",
      "Epoch: 89 \tTraining Loss: 0.662870 \tValidation Loss: 0.922639 \tAccuracy: 0.582418\n",
      "Epoch: 90 \tTraining Loss: 0.661670 \tValidation Loss: 0.922473 \tAccuracy: 0.576923\n",
      "Epoch: 91 \tTraining Loss: 0.660721 \tValidation Loss: 0.922751 \tAccuracy: 0.576923\n",
      "Epoch: 92 \tTraining Loss: 0.659864 \tValidation Loss: 0.922817 \tAccuracy: 0.576923\n",
      "Epoch: 93 \tTraining Loss: 0.659128 \tValidation Loss: 0.923119 \tAccuracy: 0.576923\n",
      "Epoch: 94 \tTraining Loss: 0.658548 \tValidation Loss: 0.923286 \tAccuracy: 0.576923\n",
      "Epoch: 95 \tTraining Loss: 0.658118 \tValidation Loss: 0.923584 \tAccuracy: 0.576923\n",
      "Epoch: 96 \tTraining Loss: 0.657772 \tValidation Loss: 0.923765 \tAccuracy: 0.576923\n",
      "Epoch: 97 \tTraining Loss: 0.657552 \tValidation Loss: 0.923901 \tAccuracy: 0.576923\n",
      "Epoch: 98 \tTraining Loss: 0.657439 \tValidation Loss: 0.924265 \tAccuracy: 0.576923\n",
      "Epoch: 99 \tTraining Loss: 0.657411 \tValidation Loss: 0.924604 \tAccuracy: 0.576923\n",
      "Epoch: 100 \tTraining Loss: 0.657471 \tValidation Loss: 0.925144 \tAccuracy: 0.576923\n",
      "Epoch: 101 \tTraining Loss: 0.657520 \tValidation Loss: 0.925801 \tAccuracy: 0.576923\n",
      "Epoch: 102 \tTraining Loss: 0.657661 \tValidation Loss: 0.926452 \tAccuracy: 0.576923\n",
      "Epoch: 103 \tTraining Loss: 0.657790 \tValidation Loss: 0.927409 \tAccuracy: 0.576923\n",
      "Epoch: 104 \tTraining Loss: 0.657975 \tValidation Loss: 0.928192 \tAccuracy: 0.582418\n",
      "Epoch: 105 \tTraining Loss: 0.658261 \tValidation Loss: 0.929407 \tAccuracy: 0.582418\n",
      "Epoch: 106 \tTraining Loss: 0.658431 \tValidation Loss: 0.930377 \tAccuracy: 0.582418\n",
      "Epoch: 107 \tTraining Loss: 0.658581 \tValidation Loss: 0.931732 \tAccuracy: 0.582418\n",
      "Epoch: 108 \tTraining Loss: 0.658827 \tValidation Loss: 0.932847 \tAccuracy: 0.587912\n",
      "Epoch: 109 \tTraining Loss: 0.658921 \tValidation Loss: 0.934158 \tAccuracy: 0.587912\n",
      "Epoch: 110 \tTraining Loss: 0.658868 \tValidation Loss: 0.935492 \tAccuracy: 0.593407\n",
      "Epoch: 111 \tTraining Loss: 0.658919 \tValidation Loss: 0.937089 \tAccuracy: 0.593407\n",
      "Epoch: 112 \tTraining Loss: 0.658586 \tValidation Loss: 0.938465 \tAccuracy: 0.587912\n",
      "Epoch: 113 \tTraining Loss: 0.658518 \tValidation Loss: 0.939358 \tAccuracy: 0.587912\n",
      "Epoch: 114 \tTraining Loss: 0.658152 \tValidation Loss: 0.942421 \tAccuracy: 0.587912\n",
      "Epoch: 115 \tTraining Loss: 0.657787 \tValidation Loss: 0.943361 \tAccuracy: 0.582418\n",
      "Epoch: 116 \tTraining Loss: 0.657194 \tValidation Loss: 0.946199 \tAccuracy: 0.582418\n",
      "Epoch: 117 \tTraining Loss: 0.656959 \tValidation Loss: 0.947348 \tAccuracy: 0.593407\n",
      "Epoch: 118 \tTraining Loss: 0.656134 \tValidation Loss: 0.949469 \tAccuracy: 0.593407\n",
      "Epoch: 119 \tTraining Loss: 0.655666 \tValidation Loss: 0.951442 \tAccuracy: 0.593407\n",
      "Epoch: 120 \tTraining Loss: 0.654615 \tValidation Loss: 0.952861 \tAccuracy: 0.593407\n",
      "Epoch: 121 \tTraining Loss: 0.653621 \tValidation Loss: 0.953993 \tAccuracy: 0.593407\n",
      "Epoch: 122 \tTraining Loss: 0.652733 \tValidation Loss: 0.954474 \tAccuracy: 0.593407\n",
      "Epoch: 123 \tTraining Loss: 0.651229 \tValidation Loss: 0.954391 \tAccuracy: 0.587912\n",
      "Epoch: 124 \tTraining Loss: 0.649890 \tValidation Loss: 0.955040 \tAccuracy: 0.587912\n",
      "Epoch: 125 \tTraining Loss: 0.648458 \tValidation Loss: 0.958532 \tAccuracy: 0.582418\n",
      "Epoch: 126 \tTraining Loss: 0.646738 \tValidation Loss: 0.959106 \tAccuracy: 0.582418\n",
      "Epoch: 127 \tTraining Loss: 0.645207 \tValidation Loss: 0.959338 \tAccuracy: 0.582418\n",
      "Epoch: 128 \tTraining Loss: 0.643702 \tValidation Loss: 0.959249 \tAccuracy: 0.587912\n",
      "Epoch: 129 \tTraining Loss: 0.641725 \tValidation Loss: 0.960366 \tAccuracy: 0.587912\n",
      "Epoch: 130 \tTraining Loss: 0.639348 \tValidation Loss: 0.961314 \tAccuracy: 0.587912\n",
      "Epoch: 131 \tTraining Loss: 0.637622 \tValidation Loss: 0.961995 \tAccuracy: 0.587912\n",
      "Epoch: 132 \tTraining Loss: 0.635009 \tValidation Loss: 0.962298 \tAccuracy: 0.593407\n",
      "Epoch: 133 \tTraining Loss: 0.633007 \tValidation Loss: 0.965155 \tAccuracy: 0.593407\n",
      "Epoch: 134 \tTraining Loss: 0.630226 \tValidation Loss: 0.966560 \tAccuracy: 0.593407\n",
      "Epoch: 135 \tTraining Loss: 0.627909 \tValidation Loss: 0.967899 \tAccuracy: 0.598901\n",
      "Epoch: 136 \tTraining Loss: 0.625819 \tValidation Loss: 0.967843 \tAccuracy: 0.587912\n",
      "Epoch: 137 \tTraining Loss: 0.623165 \tValidation Loss: 0.970453 \tAccuracy: 0.593407\n",
      "Epoch: 138 \tTraining Loss: 0.620810 \tValidation Loss: 0.971997 \tAccuracy: 0.587912\n",
      "Epoch: 139 \tTraining Loss: 0.618039 \tValidation Loss: 0.973353 \tAccuracy: 0.582418\n",
      "Epoch: 140 \tTraining Loss: 0.616176 \tValidation Loss: 0.973373 \tAccuracy: 0.582418\n",
      "Epoch: 141 \tTraining Loss: 0.613600 \tValidation Loss: 0.975731 \tAccuracy: 0.582418\n",
      "Epoch: 142 \tTraining Loss: 0.611820 \tValidation Loss: 0.977653 \tAccuracy: 0.582418\n",
      "Epoch: 143 \tTraining Loss: 0.609277 \tValidation Loss: 0.979183 \tAccuracy: 0.582418\n",
      "Epoch: 144 \tTraining Loss: 0.607148 \tValidation Loss: 0.981136 \tAccuracy: 0.582418\n",
      "Epoch: 145 \tTraining Loss: 0.605386 \tValidation Loss: 0.980750 \tAccuracy: 0.582418\n",
      "Epoch: 146 \tTraining Loss: 0.603448 \tValidation Loss: 0.982173 \tAccuracy: 0.576923\n",
      "Epoch: 147 \tTraining Loss: 0.601274 \tValidation Loss: 0.983815 \tAccuracy: 0.576923\n",
      "Epoch: 148 \tTraining Loss: 0.599657 \tValidation Loss: 0.984663 \tAccuracy: 0.576923\n",
      "Epoch: 149 \tTraining Loss: 0.597780 \tValidation Loss: 0.984998 \tAccuracy: 0.576923\n",
      "Epoch: 150 \tTraining Loss: 0.595955 \tValidation Loss: 0.985018 \tAccuracy: 0.576923\n",
      "Epoch: 151 \tTraining Loss: 0.594485 \tValidation Loss: 0.985463 \tAccuracy: 0.576923\n",
      "Epoch: 152 \tTraining Loss: 0.592996 \tValidation Loss: 0.985571 \tAccuracy: 0.576923\n",
      "Epoch: 153 \tTraining Loss: 0.591718 \tValidation Loss: 0.985948 \tAccuracy: 0.576923\n",
      "Epoch: 154 \tTraining Loss: 0.590427 \tValidation Loss: 0.986432 \tAccuracy: 0.576923\n",
      "Epoch: 155 \tTraining Loss: 0.589425 \tValidation Loss: 0.986468 \tAccuracy: 0.576923\n",
      "Epoch: 156 \tTraining Loss: 0.588472 \tValidation Loss: 0.986533 \tAccuracy: 0.576923\n",
      "Epoch: 157 \tTraining Loss: 0.587702 \tValidation Loss: 0.987173 \tAccuracy: 0.576923\n",
      "Epoch: 158 \tTraining Loss: 0.587192 \tValidation Loss: 0.987113 \tAccuracy: 0.576923\n",
      "Epoch: 159 \tTraining Loss: 0.586720 \tValidation Loss: 0.987345 \tAccuracy: 0.576923\n",
      "Epoch: 160 \tTraining Loss: 0.586414 \tValidation Loss: 0.987825 \tAccuracy: 0.576923\n",
      "Epoch: 161 \tTraining Loss: 0.586288 \tValidation Loss: 0.987768 \tAccuracy: 0.576923\n",
      "Epoch: 162 \tTraining Loss: 0.586199 \tValidation Loss: 0.988536 \tAccuracy: 0.571429\n",
      "Epoch: 163 \tTraining Loss: 0.586295 \tValidation Loss: 0.988902 \tAccuracy: 0.571429\n",
      "Epoch: 164 \tTraining Loss: 0.586440 \tValidation Loss: 0.989646 \tAccuracy: 0.571429\n",
      "Epoch: 165 \tTraining Loss: 0.586748 \tValidation Loss: 0.989941 \tAccuracy: 0.571429\n",
      "Epoch: 166 \tTraining Loss: 0.587148 \tValidation Loss: 0.990775 \tAccuracy: 0.571429\n",
      "Epoch: 167 \tTraining Loss: 0.587471 \tValidation Loss: 0.991796 \tAccuracy: 0.565934\n",
      "Epoch: 168 \tTraining Loss: 0.588043 \tValidation Loss: 0.992639 \tAccuracy: 0.565934\n",
      "Epoch: 169 \tTraining Loss: 0.588509 \tValidation Loss: 0.993422 \tAccuracy: 0.565934\n",
      "Epoch: 170 \tTraining Loss: 0.588999 \tValidation Loss: 0.993868 \tAccuracy: 0.565934\n",
      "Epoch: 171 \tTraining Loss: 0.589565 \tValidation Loss: 0.994763 \tAccuracy: 0.571429\n",
      "Epoch: 172 \tTraining Loss: 0.589984 \tValidation Loss: 0.996461 \tAccuracy: 0.571429\n",
      "Epoch: 173 \tTraining Loss: 0.590624 \tValidation Loss: 0.996573 \tAccuracy: 0.571429\n",
      "Epoch: 174 \tTraining Loss: 0.590688 \tValidation Loss: 0.997123 \tAccuracy: 0.571429\n",
      "Epoch: 175 \tTraining Loss: 0.591475 \tValidation Loss: 0.998361 \tAccuracy: 0.571429\n",
      "Epoch: 176 \tTraining Loss: 0.591799 \tValidation Loss: 0.998352 \tAccuracy: 0.571429\n",
      "Epoch: 177 \tTraining Loss: 0.591754 \tValidation Loss: 0.999170 \tAccuracy: 0.576923\n",
      "Epoch: 178 \tTraining Loss: 0.591870 \tValidation Loss: 1.001925 \tAccuracy: 0.576923\n",
      "Epoch: 179 \tTraining Loss: 0.592277 \tValidation Loss: 1.000637 \tAccuracy: 0.576923\n",
      "Epoch: 180 \tTraining Loss: 0.591915 \tValidation Loss: 1.003195 \tAccuracy: 0.576923\n",
      "Epoch: 181 \tTraining Loss: 0.591747 \tValidation Loss: 1.002762 \tAccuracy: 0.576923\n",
      "Epoch: 182 \tTraining Loss: 0.591241 \tValidation Loss: 1.003658 \tAccuracy: 0.576923\n",
      "Epoch: 183 \tTraining Loss: 0.591103 \tValidation Loss: 1.006821 \tAccuracy: 0.576923\n",
      "Epoch: 184 \tTraining Loss: 0.590966 \tValidation Loss: 1.006402 \tAccuracy: 0.582418\n",
      "Epoch: 185 \tTraining Loss: 0.590506 \tValidation Loss: 1.009805 \tAccuracy: 0.582418\n",
      "Epoch: 186 \tTraining Loss: 0.590062 \tValidation Loss: 1.009443 \tAccuracy: 0.582418\n",
      "Epoch: 187 \tTraining Loss: 0.588789 \tValidation Loss: 1.013325 \tAccuracy: 0.582418\n",
      "Epoch: 188 \tTraining Loss: 0.588635 \tValidation Loss: 1.011139 \tAccuracy: 0.582418\n",
      "Epoch: 189 \tTraining Loss: 0.586886 \tValidation Loss: 1.013283 \tAccuracy: 0.582418\n",
      "Epoch: 190 \tTraining Loss: 0.585310 \tValidation Loss: 1.011585 \tAccuracy: 0.582418\n",
      "Epoch: 191 \tTraining Loss: 0.584429 \tValidation Loss: 1.013341 \tAccuracy: 0.582418\n",
      "Epoch: 192 \tTraining Loss: 0.582931 \tValidation Loss: 1.014145 \tAccuracy: 0.582418\n",
      "Epoch: 193 \tTraining Loss: 0.580431 \tValidation Loss: 1.016354 \tAccuracy: 0.582418\n",
      "Epoch: 194 \tTraining Loss: 0.579540 \tValidation Loss: 1.016736 \tAccuracy: 0.582418\n",
      "Epoch: 195 \tTraining Loss: 0.577329 \tValidation Loss: 1.018834 \tAccuracy: 0.582418\n",
      "Epoch: 196 \tTraining Loss: 0.574645 \tValidation Loss: 1.018215 \tAccuracy: 0.582418\n",
      "Epoch: 197 \tTraining Loss: 0.572893 \tValidation Loss: 1.021662 \tAccuracy: 0.587912\n",
      "Epoch: 198 \tTraining Loss: 0.570662 \tValidation Loss: 1.021633 \tAccuracy: 0.582418\n",
      "Epoch: 199 \tTraining Loss: 0.568236 \tValidation Loss: 1.021585 \tAccuracy: 0.582418\n",
      "Epoch: 200 \tTraining Loss: 0.566156 \tValidation Loss: 1.024731 \tAccuracy: 0.582418\n",
      "Epoch: 201 \tTraining Loss: 0.563914 \tValidation Loss: 1.027680 \tAccuracy: 0.582418\n",
      "Epoch: 202 \tTraining Loss: 0.561713 \tValidation Loss: 1.028493 \tAccuracy: 0.582418\n",
      "Epoch: 203 \tTraining Loss: 0.559427 \tValidation Loss: 1.027623 \tAccuracy: 0.582418\n",
      "Epoch: 204 \tTraining Loss: 0.556805 \tValidation Loss: 1.029422 \tAccuracy: 0.582418\n",
      "Epoch: 205 \tTraining Loss: 0.554522 \tValidation Loss: 1.030768 \tAccuracy: 0.582418\n",
      "Epoch: 206 \tTraining Loss: 0.552091 \tValidation Loss: 1.032592 \tAccuracy: 0.582418\n",
      "Epoch: 207 \tTraining Loss: 0.549706 \tValidation Loss: 1.033175 \tAccuracy: 0.582418\n",
      "Epoch: 208 \tTraining Loss: 0.547568 \tValidation Loss: 1.034204 \tAccuracy: 0.582418\n",
      "Epoch: 209 \tTraining Loss: 0.545246 \tValidation Loss: 1.034210 \tAccuracy: 0.582418\n",
      "Epoch: 210 \tTraining Loss: 0.543171 \tValidation Loss: 1.035773 \tAccuracy: 0.582418\n",
      "Epoch: 211 \tTraining Loss: 0.541175 \tValidation Loss: 1.037406 \tAccuracy: 0.582418\n",
      "Epoch: 212 \tTraining Loss: 0.538980 \tValidation Loss: 1.036797 \tAccuracy: 0.576923\n",
      "Epoch: 213 \tTraining Loss: 0.537207 \tValidation Loss: 1.037526 \tAccuracy: 0.571429\n",
      "Epoch: 214 \tTraining Loss: 0.535372 \tValidation Loss: 1.038138 \tAccuracy: 0.571429\n",
      "Epoch: 215 \tTraining Loss: 0.533705 \tValidation Loss: 1.038020 \tAccuracy: 0.571429\n",
      "Epoch: 216 \tTraining Loss: 0.531949 \tValidation Loss: 1.038189 \tAccuracy: 0.571429\n",
      "Epoch: 217 \tTraining Loss: 0.530682 \tValidation Loss: 1.038151 \tAccuracy: 0.571429\n",
      "Epoch: 218 \tTraining Loss: 0.529306 \tValidation Loss: 1.038758 \tAccuracy: 0.571429\n",
      "Epoch: 219 \tTraining Loss: 0.528330 \tValidation Loss: 1.039212 \tAccuracy: 0.571429\n",
      "Epoch: 220 \tTraining Loss: 0.527223 \tValidation Loss: 1.038996 \tAccuracy: 0.571429\n",
      "Epoch: 221 \tTraining Loss: 0.526487 \tValidation Loss: 1.039312 \tAccuracy: 0.571429\n",
      "Epoch: 222 \tTraining Loss: 0.525736 \tValidation Loss: 1.039560 \tAccuracy: 0.571429\n",
      "Epoch: 223 \tTraining Loss: 0.525401 \tValidation Loss: 1.039849 \tAccuracy: 0.571429\n",
      "Epoch: 224 \tTraining Loss: 0.525037 \tValidation Loss: 1.040090 \tAccuracy: 0.571429\n",
      "Epoch: 225 \tTraining Loss: 0.524881 \tValidation Loss: 1.040394 \tAccuracy: 0.571429\n",
      "Epoch: 226 \tTraining Loss: 0.524857 \tValidation Loss: 1.040592 \tAccuracy: 0.571429\n",
      "Epoch: 227 \tTraining Loss: 0.524970 \tValidation Loss: 1.040897 \tAccuracy: 0.571429\n",
      "Epoch: 228 \tTraining Loss: 0.525310 \tValidation Loss: 1.041052 \tAccuracy: 0.571429\n",
      "Epoch: 229 \tTraining Loss: 0.525750 \tValidation Loss: 1.041883 \tAccuracy: 0.571429\n",
      "Epoch: 230 \tTraining Loss: 0.526050 \tValidation Loss: 1.042292 \tAccuracy: 0.571429\n",
      "Epoch: 231 \tTraining Loss: 0.526704 \tValidation Loss: 1.042973 \tAccuracy: 0.571429\n",
      "Epoch: 232 \tTraining Loss: 0.527082 \tValidation Loss: 1.043307 \tAccuracy: 0.571429\n",
      "Epoch: 233 \tTraining Loss: 0.527938 \tValidation Loss: 1.044607 \tAccuracy: 0.571429\n",
      "Epoch: 234 \tTraining Loss: 0.528740 \tValidation Loss: 1.045179 \tAccuracy: 0.571429\n",
      "Epoch: 235 \tTraining Loss: 0.528949 \tValidation Loss: 1.044796 \tAccuracy: 0.571429\n",
      "Epoch: 236 \tTraining Loss: 0.530151 \tValidation Loss: 1.045108 \tAccuracy: 0.565934\n",
      "Epoch: 237 \tTraining Loss: 0.530708 \tValidation Loss: 1.045713 \tAccuracy: 0.565934\n",
      "Epoch: 238 \tTraining Loss: 0.531669 \tValidation Loss: 1.047554 \tAccuracy: 0.565934\n",
      "Epoch: 239 \tTraining Loss: 0.532046 \tValidation Loss: 1.047590 \tAccuracy: 0.565934\n",
      "Epoch: 240 \tTraining Loss: 0.532532 \tValidation Loss: 1.046400 \tAccuracy: 0.571429\n",
      "Epoch: 241 \tTraining Loss: 0.533832 \tValidation Loss: 1.047713 \tAccuracy: 0.571429\n",
      "Epoch: 242 \tTraining Loss: 0.533783 \tValidation Loss: 1.048759 \tAccuracy: 0.571429\n",
      "Epoch: 243 \tTraining Loss: 0.534165 \tValidation Loss: 1.048272 \tAccuracy: 0.571429\n",
      "Epoch: 244 \tTraining Loss: 0.534527 \tValidation Loss: 1.050426 \tAccuracy: 0.565934\n",
      "Epoch: 245 \tTraining Loss: 0.534218 \tValidation Loss: 1.050780 \tAccuracy: 0.571429\n",
      "Epoch: 246 \tTraining Loss: 0.535590 \tValidation Loss: 1.051602 \tAccuracy: 0.565934\n",
      "Epoch: 247 \tTraining Loss: 0.535729 \tValidation Loss: 1.052726 \tAccuracy: 0.565934\n",
      "Epoch: 248 \tTraining Loss: 0.534798 \tValidation Loss: 1.052582 \tAccuracy: 0.565934\n",
      "Epoch: 249 \tTraining Loss: 0.535467 \tValidation Loss: 1.053144 \tAccuracy: 0.565934\n",
      "Epoch: 0 \tTraining Loss: 1.084336 \tValidation Loss: 1.061960 \tAccuracy: 0.450549\n",
      "Better loss found. (inf --> 1.061960).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.037508 \tValidation Loss: 1.035255 \tAccuracy: 0.417582\n",
      "Better loss found. (1.061960 --> 1.035255).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.004699 \tValidation Loss: 1.013746 \tAccuracy: 0.467033\n",
      "Better loss found. (1.035255 --> 1.013746).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.974119 \tValidation Loss: 0.995623 \tAccuracy: 0.483516\n",
      "Better loss found. (1.013746 --> 0.995623).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.946295 \tValidation Loss: 0.980161 \tAccuracy: 0.472527\n",
      "Better loss found. (0.995623 --> 0.980161).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.923190 \tValidation Loss: 0.970153 \tAccuracy: 0.489011\n",
      "Better loss found. (0.980161 --> 0.970153).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.905531 \tValidation Loss: 0.961565 \tAccuracy: 0.494505\n",
      "Better loss found. (0.970153 --> 0.961565).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.891707 \tValidation Loss: 0.954453 \tAccuracy: 0.510989\n",
      "Better loss found. (0.961565 --> 0.954453).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.880307 \tValidation Loss: 0.949255 \tAccuracy: 0.510989\n",
      "Better loss found. (0.954453 --> 0.949255).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.870511 \tValidation Loss: 0.945469 \tAccuracy: 0.505495\n",
      "Better loss found. (0.949255 --> 0.945469).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.862193 \tValidation Loss: 0.942003 \tAccuracy: 0.510989\n",
      "Better loss found. (0.945469 --> 0.942003).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.854654 \tValidation Loss: 0.938792 \tAccuracy: 0.521978\n",
      "Better loss found. (0.942003 --> 0.938792).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.848105 \tValidation Loss: 0.936070 \tAccuracy: 0.527473\n",
      "Better loss found. (0.938792 --> 0.936070).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.842056 \tValidation Loss: 0.933378 \tAccuracy: 0.543956\n",
      "Better loss found. (0.936070 --> 0.933378).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.836805 \tValidation Loss: 0.930629 \tAccuracy: 0.538462\n",
      "Better loss found. (0.933378 --> 0.930629).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.831993 \tValidation Loss: 0.928422 \tAccuracy: 0.543956\n",
      "Better loss found. (0.930629 --> 0.928422).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.827637 \tValidation Loss: 0.926189 \tAccuracy: 0.543956\n",
      "Better loss found. (0.928422 --> 0.926189).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.823633 \tValidation Loss: 0.924546 \tAccuracy: 0.543956\n",
      "Better loss found. (0.926189 --> 0.924546).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.819933 \tValidation Loss: 0.922966 \tAccuracy: 0.549451\n",
      "Better loss found. (0.924546 --> 0.922966).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.816678 \tValidation Loss: 0.921826 \tAccuracy: 0.554945\n",
      "Better loss found. (0.922966 --> 0.921826).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.813704 \tValidation Loss: 0.920773 \tAccuracy: 0.554945\n",
      "Better loss found. (0.921826 --> 0.920773).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.811010 \tValidation Loss: 0.920061 \tAccuracy: 0.554945\n",
      "Better loss found. (0.920773 --> 0.920061).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.808678 \tValidation Loss: 0.919160 \tAccuracy: 0.554945\n",
      "Better loss found. (0.920061 --> 0.919160).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.806577 \tValidation Loss: 0.918327 \tAccuracy: 0.554945\n",
      "Better loss found. (0.919160 --> 0.918327).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.804657 \tValidation Loss: 0.917791 \tAccuracy: 0.549451\n",
      "Better loss found. (0.918327 --> 0.917791).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.803010 \tValidation Loss: 0.917174 \tAccuracy: 0.549451\n",
      "Better loss found. (0.917791 --> 0.917174).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.801568 \tValidation Loss: 0.916727 \tAccuracy: 0.549451\n",
      "Better loss found. (0.917174 --> 0.916727).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.800315 \tValidation Loss: 0.916317 \tAccuracy: 0.549451\n",
      "Better loss found. (0.916727 --> 0.916317).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.799250 \tValidation Loss: 0.915996 \tAccuracy: 0.549451\n",
      "Better loss found. (0.916317 --> 0.915996).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.798357 \tValidation Loss: 0.915680 \tAccuracy: 0.549451\n",
      "Better loss found. (0.915996 --> 0.915680).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.797573 \tValidation Loss: 0.915438 \tAccuracy: 0.554945\n",
      "Better loss found. (0.915680 --> 0.915438).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.796929 \tValidation Loss: 0.915193 \tAccuracy: 0.549451\n",
      "Better loss found. (0.915438 --> 0.915193).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.796366 \tValidation Loss: 0.914981 \tAccuracy: 0.549451\n",
      "Better loss found. (0.915193 --> 0.914981).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.795836 \tValidation Loss: 0.914812 \tAccuracy: 0.549451\n",
      "Better loss found. (0.914981 --> 0.914812).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.795383 \tValidation Loss: 0.914650 \tAccuracy: 0.549451\n",
      "Better loss found. (0.914812 --> 0.914650).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.794912 \tValidation Loss: 0.914485 \tAccuracy: 0.549451\n",
      "Better loss found. (0.914650 --> 0.914485).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.794484 \tValidation Loss: 0.914328 \tAccuracy: 0.549451\n",
      "Better loss found. (0.914485 --> 0.914328).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.793951 \tValidation Loss: 0.914163 \tAccuracy: 0.549451\n",
      "Better loss found. (0.914328 --> 0.914163).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.793414 \tValidation Loss: 0.914134 \tAccuracy: 0.554945\n",
      "Better loss found. (0.914163 --> 0.914134).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.792759 \tValidation Loss: 0.914068 \tAccuracy: 0.554945\n",
      "Better loss found. (0.914134 --> 0.914068).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.792052 \tValidation Loss: 0.914165 \tAccuracy: 0.554945\n",
      "Epoch: 41 \tTraining Loss: 0.791243 \tValidation Loss: 0.914188 \tAccuracy: 0.554945\n",
      "Epoch: 42 \tTraining Loss: 0.790249 \tValidation Loss: 0.914521 \tAccuracy: 0.549451\n",
      "Epoch: 43 \tTraining Loss: 0.789129 \tValidation Loss: 0.914565 \tAccuracy: 0.549451\n",
      "Epoch: 44 \tTraining Loss: 0.787971 \tValidation Loss: 0.914651 \tAccuracy: 0.549451\n",
      "Epoch: 45 \tTraining Loss: 0.786500 \tValidation Loss: 0.914728 \tAccuracy: 0.549451\n",
      "Epoch: 46 \tTraining Loss: 0.785098 \tValidation Loss: 0.914830 \tAccuracy: 0.554945\n",
      "Epoch: 47 \tTraining Loss: 0.783501 \tValidation Loss: 0.914933 \tAccuracy: 0.554945\n",
      "Epoch: 48 \tTraining Loss: 0.781736 \tValidation Loss: 0.914730 \tAccuracy: 0.549451\n",
      "Epoch: 49 \tTraining Loss: 0.779670 \tValidation Loss: 0.914677 \tAccuracy: 0.549451\n",
      "Epoch: 50 \tTraining Loss: 0.777408 \tValidation Loss: 0.915297 \tAccuracy: 0.560440\n",
      "Epoch: 51 \tTraining Loss: 0.775023 \tValidation Loss: 0.915289 \tAccuracy: 0.560440\n",
      "Epoch: 52 \tTraining Loss: 0.772661 \tValidation Loss: 0.915096 \tAccuracy: 0.560440\n",
      "Epoch: 53 \tTraining Loss: 0.769688 \tValidation Loss: 0.915681 \tAccuracy: 0.549451\n",
      "Epoch: 54 \tTraining Loss: 0.766986 \tValidation Loss: 0.915938 \tAccuracy: 0.549451\n",
      "Epoch: 55 \tTraining Loss: 0.763987 \tValidation Loss: 0.915988 \tAccuracy: 0.549451\n",
      "Epoch: 56 \tTraining Loss: 0.761025 \tValidation Loss: 0.915911 \tAccuracy: 0.554945\n",
      "Epoch: 57 \tTraining Loss: 0.758031 \tValidation Loss: 0.915986 \tAccuracy: 0.560440\n",
      "Epoch: 58 \tTraining Loss: 0.755383 \tValidation Loss: 0.917099 \tAccuracy: 0.560440\n",
      "Epoch: 59 \tTraining Loss: 0.752308 \tValidation Loss: 0.917298 \tAccuracy: 0.565934\n",
      "Epoch: 60 \tTraining Loss: 0.749116 \tValidation Loss: 0.917469 \tAccuracy: 0.560440\n",
      "Epoch: 61 \tTraining Loss: 0.745942 \tValidation Loss: 0.918558 \tAccuracy: 0.554945\n",
      "Epoch: 62 \tTraining Loss: 0.742895 \tValidation Loss: 0.919090 \tAccuracy: 0.549451\n",
      "Epoch: 63 \tTraining Loss: 0.740225 \tValidation Loss: 0.920499 \tAccuracy: 0.549451\n",
      "Epoch: 64 \tTraining Loss: 0.737405 \tValidation Loss: 0.920072 \tAccuracy: 0.565934\n",
      "Epoch: 65 \tTraining Loss: 0.734588 \tValidation Loss: 0.921198 \tAccuracy: 0.571429\n",
      "Epoch: 66 \tTraining Loss: 0.731899 \tValidation Loss: 0.921823 \tAccuracy: 0.571429\n",
      "Epoch: 67 \tTraining Loss: 0.728982 \tValidation Loss: 0.921570 \tAccuracy: 0.571429\n",
      "Epoch: 68 \tTraining Loss: 0.726379 \tValidation Loss: 0.922508 \tAccuracy: 0.571429\n",
      "Epoch: 69 \tTraining Loss: 0.723498 \tValidation Loss: 0.922436 \tAccuracy: 0.582418\n",
      "Epoch: 70 \tTraining Loss: 0.720561 \tValidation Loss: 0.922394 \tAccuracy: 0.582418\n",
      "Epoch: 71 \tTraining Loss: 0.717902 \tValidation Loss: 0.923526 \tAccuracy: 0.582418\n",
      "Epoch: 72 \tTraining Loss: 0.715381 \tValidation Loss: 0.922632 \tAccuracy: 0.582418\n",
      "Epoch: 73 \tTraining Loss: 0.712677 \tValidation Loss: 0.924039 \tAccuracy: 0.582418\n",
      "Epoch: 74 \tTraining Loss: 0.710106 \tValidation Loss: 0.923341 \tAccuracy: 0.582418\n",
      "Epoch: 75 \tTraining Loss: 0.707836 \tValidation Loss: 0.922917 \tAccuracy: 0.582418\n",
      "Epoch: 76 \tTraining Loss: 0.705432 \tValidation Loss: 0.924703 \tAccuracy: 0.582418\n",
      "Epoch: 77 \tTraining Loss: 0.703384 \tValidation Loss: 0.923779 \tAccuracy: 0.576923\n",
      "Epoch: 78 \tTraining Loss: 0.701092 \tValidation Loss: 0.923233 \tAccuracy: 0.576923\n",
      "Epoch: 79 \tTraining Loss: 0.698863 \tValidation Loss: 0.922206 \tAccuracy: 0.576923\n",
      "Epoch: 80 \tTraining Loss: 0.697017 \tValidation Loss: 0.921864 \tAccuracy: 0.576923\n",
      "Epoch: 81 \tTraining Loss: 0.695005 \tValidation Loss: 0.922549 \tAccuracy: 0.571429\n",
      "Epoch: 82 \tTraining Loss: 0.692852 \tValidation Loss: 0.921812 \tAccuracy: 0.571429\n",
      "Epoch: 83 \tTraining Loss: 0.691159 \tValidation Loss: 0.921437 \tAccuracy: 0.571429\n",
      "Epoch: 84 \tTraining Loss: 0.689555 \tValidation Loss: 0.921122 \tAccuracy: 0.571429\n",
      "Epoch: 85 \tTraining Loss: 0.687877 \tValidation Loss: 0.920691 \tAccuracy: 0.571429\n",
      "Epoch: 86 \tTraining Loss: 0.686478 \tValidation Loss: 0.920976 \tAccuracy: 0.571429\n",
      "Epoch: 87 \tTraining Loss: 0.685002 \tValidation Loss: 0.920740 \tAccuracy: 0.571429\n",
      "Epoch: 88 \tTraining Loss: 0.683670 \tValidation Loss: 0.920351 \tAccuracy: 0.571429\n",
      "Epoch: 89 \tTraining Loss: 0.682558 \tValidation Loss: 0.920336 \tAccuracy: 0.571429\n",
      "Epoch: 90 \tTraining Loss: 0.681443 \tValidation Loss: 0.920090 \tAccuracy: 0.571429\n",
      "Epoch: 91 \tTraining Loss: 0.680563 \tValidation Loss: 0.919860 \tAccuracy: 0.576923\n",
      "Epoch: 92 \tTraining Loss: 0.679862 \tValidation Loss: 0.919851 \tAccuracy: 0.576923\n",
      "Epoch: 93 \tTraining Loss: 0.679198 \tValidation Loss: 0.919676 \tAccuracy: 0.576923\n",
      "Epoch: 94 \tTraining Loss: 0.678707 \tValidation Loss: 0.919870 \tAccuracy: 0.576923\n",
      "Epoch: 95 \tTraining Loss: 0.678368 \tValidation Loss: 0.919875 \tAccuracy: 0.576923\n",
      "Epoch: 96 \tTraining Loss: 0.677989 \tValidation Loss: 0.919974 \tAccuracy: 0.582418\n",
      "Epoch: 97 \tTraining Loss: 0.677814 \tValidation Loss: 0.920110 \tAccuracy: 0.582418\n",
      "Epoch: 98 \tTraining Loss: 0.677646 \tValidation Loss: 0.920304 \tAccuracy: 0.582418\n",
      "Epoch: 99 \tTraining Loss: 0.677580 \tValidation Loss: 0.920528 \tAccuracy: 0.582418\n",
      "Epoch: 100 \tTraining Loss: 0.677542 \tValidation Loss: 0.920864 \tAccuracy: 0.582418\n",
      "Epoch: 101 \tTraining Loss: 0.677570 \tValidation Loss: 0.921406 \tAccuracy: 0.576923\n",
      "Epoch: 102 \tTraining Loss: 0.677590 \tValidation Loss: 0.921695 \tAccuracy: 0.576923\n",
      "Epoch: 103 \tTraining Loss: 0.677677 \tValidation Loss: 0.922258 \tAccuracy: 0.571429\n",
      "Epoch: 104 \tTraining Loss: 0.677666 \tValidation Loss: 0.923514 \tAccuracy: 0.576923\n",
      "Epoch: 105 \tTraining Loss: 0.677828 \tValidation Loss: 0.924020 \tAccuracy: 0.576923\n",
      "Epoch: 106 \tTraining Loss: 0.677844 \tValidation Loss: 0.924749 \tAccuracy: 0.576923\n",
      "Epoch: 107 \tTraining Loss: 0.677914 \tValidation Loss: 0.925804 \tAccuracy: 0.576923\n",
      "Epoch: 108 \tTraining Loss: 0.677746 \tValidation Loss: 0.927110 \tAccuracy: 0.576923\n",
      "Epoch: 109 \tTraining Loss: 0.677716 \tValidation Loss: 0.927131 \tAccuracy: 0.576923\n",
      "Epoch: 110 \tTraining Loss: 0.677578 \tValidation Loss: 0.929114 \tAccuracy: 0.576923\n",
      "Epoch: 111 \tTraining Loss: 0.677348 \tValidation Loss: 0.929363 \tAccuracy: 0.576923\n",
      "Epoch: 112 \tTraining Loss: 0.677020 \tValidation Loss: 0.930338 \tAccuracy: 0.576923\n",
      "Epoch: 113 \tTraining Loss: 0.676473 \tValidation Loss: 0.931742 \tAccuracy: 0.576923\n",
      "Epoch: 114 \tTraining Loss: 0.675944 \tValidation Loss: 0.933925 \tAccuracy: 0.582418\n",
      "Epoch: 115 \tTraining Loss: 0.675758 \tValidation Loss: 0.935967 \tAccuracy: 0.571429\n",
      "Epoch: 116 \tTraining Loss: 0.674769 \tValidation Loss: 0.935935 \tAccuracy: 0.571429\n",
      "Epoch: 117 \tTraining Loss: 0.673943 \tValidation Loss: 0.939004 \tAccuracy: 0.576923\n",
      "Epoch: 118 \tTraining Loss: 0.672991 \tValidation Loss: 0.939300 \tAccuracy: 0.582418\n",
      "Epoch: 119 \tTraining Loss: 0.671822 \tValidation Loss: 0.941736 \tAccuracy: 0.571429\n",
      "Epoch: 120 \tTraining Loss: 0.670687 \tValidation Loss: 0.941062 \tAccuracy: 0.571429\n",
      "Epoch: 121 \tTraining Loss: 0.669678 \tValidation Loss: 0.942910 \tAccuracy: 0.565934\n",
      "Epoch: 122 \tTraining Loss: 0.667758 \tValidation Loss: 0.944452 \tAccuracy: 0.565934\n",
      "Epoch: 123 \tTraining Loss: 0.666341 \tValidation Loss: 0.948205 \tAccuracy: 0.571429\n",
      "Epoch: 124 \tTraining Loss: 0.664008 \tValidation Loss: 0.947600 \tAccuracy: 0.565934\n",
      "Epoch: 125 \tTraining Loss: 0.662437 \tValidation Loss: 0.951494 \tAccuracy: 0.565934\n",
      "Epoch: 126 \tTraining Loss: 0.659864 \tValidation Loss: 0.949494 \tAccuracy: 0.565934\n",
      "Epoch: 127 \tTraining Loss: 0.657176 \tValidation Loss: 0.952413 \tAccuracy: 0.565934\n",
      "Epoch: 128 \tTraining Loss: 0.654964 \tValidation Loss: 0.954618 \tAccuracy: 0.565934\n",
      "Epoch: 129 \tTraining Loss: 0.652063 \tValidation Loss: 0.957541 \tAccuracy: 0.565934\n",
      "Epoch: 130 \tTraining Loss: 0.649578 \tValidation Loss: 0.958181 \tAccuracy: 0.565934\n",
      "Epoch: 131 \tTraining Loss: 0.647337 \tValidation Loss: 0.960898 \tAccuracy: 0.565934\n",
      "Epoch: 132 \tTraining Loss: 0.644382 \tValidation Loss: 0.962820 \tAccuracy: 0.565934\n",
      "Epoch: 133 \tTraining Loss: 0.642116 \tValidation Loss: 0.963178 \tAccuracy: 0.565934\n",
      "Epoch: 134 \tTraining Loss: 0.639505 \tValidation Loss: 0.964283 \tAccuracy: 0.565934\n",
      "Epoch: 135 \tTraining Loss: 0.637166 \tValidation Loss: 0.964597 \tAccuracy: 0.565934\n",
      "Epoch: 136 \tTraining Loss: 0.634183 \tValidation Loss: 0.967283 \tAccuracy: 0.565934\n",
      "Epoch: 137 \tTraining Loss: 0.631746 \tValidation Loss: 0.967239 \tAccuracy: 0.565934\n",
      "Epoch: 138 \tTraining Loss: 0.628913 \tValidation Loss: 0.969042 \tAccuracy: 0.565934\n",
      "Epoch: 139 \tTraining Loss: 0.627120 \tValidation Loss: 0.969505 \tAccuracy: 0.565934\n",
      "Epoch: 140 \tTraining Loss: 0.624194 \tValidation Loss: 0.971530 \tAccuracy: 0.565934\n",
      "Epoch: 141 \tTraining Loss: 0.621950 \tValidation Loss: 0.972666 \tAccuracy: 0.565934\n",
      "Epoch: 142 \tTraining Loss: 0.619769 \tValidation Loss: 0.973526 \tAccuracy: 0.565934\n",
      "Epoch: 143 \tTraining Loss: 0.617230 \tValidation Loss: 0.974549 \tAccuracy: 0.565934\n",
      "Epoch: 144 \tTraining Loss: 0.614674 \tValidation Loss: 0.974932 \tAccuracy: 0.565934\n",
      "Epoch: 145 \tTraining Loss: 0.612520 \tValidation Loss: 0.976164 \tAccuracy: 0.565934\n",
      "Epoch: 146 \tTraining Loss: 0.610304 \tValidation Loss: 0.978272 \tAccuracy: 0.565934\n",
      "Epoch: 147 \tTraining Loss: 0.608136 \tValidation Loss: 0.976970 \tAccuracy: 0.565934\n",
      "Epoch: 148 \tTraining Loss: 0.606398 \tValidation Loss: 0.978485 \tAccuracy: 0.565934\n",
      "Epoch: 149 \tTraining Loss: 0.604446 \tValidation Loss: 0.978538 \tAccuracy: 0.560440\n",
      "Epoch: 150 \tTraining Loss: 0.602761 \tValidation Loss: 0.978830 \tAccuracy: 0.560440\n",
      "Epoch: 151 \tTraining Loss: 0.601166 \tValidation Loss: 0.979549 \tAccuracy: 0.560440\n",
      "Epoch: 152 \tTraining Loss: 0.599827 \tValidation Loss: 0.979660 \tAccuracy: 0.571429\n",
      "Epoch: 153 \tTraining Loss: 0.598592 \tValidation Loss: 0.979904 \tAccuracy: 0.571429\n",
      "Epoch: 154 \tTraining Loss: 0.597398 \tValidation Loss: 0.980081 \tAccuracy: 0.571429\n",
      "Epoch: 155 \tTraining Loss: 0.596456 \tValidation Loss: 0.980345 \tAccuracy: 0.565934\n",
      "Epoch: 156 \tTraining Loss: 0.595622 \tValidation Loss: 0.980424 \tAccuracy: 0.571429\n",
      "Epoch: 157 \tTraining Loss: 0.594955 \tValidation Loss: 0.980736 \tAccuracy: 0.565934\n",
      "Epoch: 158 \tTraining Loss: 0.594350 \tValidation Loss: 0.981155 \tAccuracy: 0.571429\n",
      "Epoch: 159 \tTraining Loss: 0.593957 \tValidation Loss: 0.981426 \tAccuracy: 0.565934\n",
      "Epoch: 160 \tTraining Loss: 0.593674 \tValidation Loss: 0.981503 \tAccuracy: 0.571429\n",
      "Epoch: 161 \tTraining Loss: 0.593386 \tValidation Loss: 0.981760 \tAccuracy: 0.565934\n",
      "Epoch: 162 \tTraining Loss: 0.593319 \tValidation Loss: 0.981958 \tAccuracy: 0.565934\n",
      "Epoch: 163 \tTraining Loss: 0.593237 \tValidation Loss: 0.982480 \tAccuracy: 0.565934\n",
      "Epoch: 164 \tTraining Loss: 0.593275 \tValidation Loss: 0.982931 \tAccuracy: 0.571429\n",
      "Epoch: 165 \tTraining Loss: 0.593405 \tValidation Loss: 0.983249 \tAccuracy: 0.571429\n",
      "Epoch: 166 \tTraining Loss: 0.593531 \tValidation Loss: 0.984007 \tAccuracy: 0.571429\n",
      "Epoch: 167 \tTraining Loss: 0.593646 \tValidation Loss: 0.984856 \tAccuracy: 0.571429\n",
      "Epoch: 168 \tTraining Loss: 0.594035 \tValidation Loss: 0.985280 \tAccuracy: 0.571429\n",
      "Epoch: 169 \tTraining Loss: 0.594027 \tValidation Loss: 0.986591 \tAccuracy: 0.571429\n",
      "Epoch: 170 \tTraining Loss: 0.594199 \tValidation Loss: 0.987315 \tAccuracy: 0.571429\n",
      "Epoch: 171 \tTraining Loss: 0.594452 \tValidation Loss: 0.988310 \tAccuracy: 0.571429\n",
      "Epoch: 172 \tTraining Loss: 0.594454 \tValidation Loss: 0.989945 \tAccuracy: 0.565934\n",
      "Epoch: 173 \tTraining Loss: 0.594569 \tValidation Loss: 0.991252 \tAccuracy: 0.565934\n",
      "Epoch: 174 \tTraining Loss: 0.594413 \tValidation Loss: 0.994063 \tAccuracy: 0.560440\n",
      "Epoch: 175 \tTraining Loss: 0.594790 \tValidation Loss: 0.994933 \tAccuracy: 0.560440\n",
      "Epoch: 176 \tTraining Loss: 0.594409 \tValidation Loss: 0.997897 \tAccuracy: 0.565934\n",
      "Epoch: 177 \tTraining Loss: 0.594312 \tValidation Loss: 0.998607 \tAccuracy: 0.565934\n",
      "Epoch: 178 \tTraining Loss: 0.593803 \tValidation Loss: 1.001743 \tAccuracy: 0.560440\n",
      "Epoch: 179 \tTraining Loss: 0.593264 \tValidation Loss: 1.005465 \tAccuracy: 0.565934\n",
      "Epoch: 180 \tTraining Loss: 0.592492 \tValidation Loss: 1.005829 \tAccuracy: 0.565934\n",
      "Epoch: 181 \tTraining Loss: 0.592167 \tValidation Loss: 1.008724 \tAccuracy: 0.565934\n",
      "Epoch: 182 \tTraining Loss: 0.590595 \tValidation Loss: 1.012040 \tAccuracy: 0.560440\n",
      "Epoch: 183 \tTraining Loss: 0.590217 \tValidation Loss: 1.015356 \tAccuracy: 0.560440\n",
      "Epoch: 184 \tTraining Loss: 0.589147 \tValidation Loss: 1.013929 \tAccuracy: 0.554945\n",
      "Epoch: 185 \tTraining Loss: 0.587729 \tValidation Loss: 1.017417 \tAccuracy: 0.554945\n",
      "Epoch: 186 \tTraining Loss: 0.587137 \tValidation Loss: 1.021929 \tAccuracy: 0.554945\n",
      "Epoch: 187 \tTraining Loss: 0.584762 \tValidation Loss: 1.022207 \tAccuracy: 0.549451\n",
      "Epoch: 188 \tTraining Loss: 0.584729 \tValidation Loss: 1.025765 \tAccuracy: 0.554945\n",
      "Epoch: 189 \tTraining Loss: 0.581577 \tValidation Loss: 1.028567 \tAccuracy: 0.554945\n",
      "Epoch: 190 \tTraining Loss: 0.580710 \tValidation Loss: 1.030495 \tAccuracy: 0.554945\n",
      "Epoch: 191 \tTraining Loss: 0.579097 \tValidation Loss: 1.034361 \tAccuracy: 0.554945\n",
      "Epoch: 192 \tTraining Loss: 0.577329 \tValidation Loss: 1.036389 \tAccuracy: 0.549451\n",
      "Epoch: 193 \tTraining Loss: 0.575206 \tValidation Loss: 1.039162 \tAccuracy: 0.560440\n",
      "Epoch: 194 \tTraining Loss: 0.574188 \tValidation Loss: 1.039232 \tAccuracy: 0.560440\n",
      "Epoch: 195 \tTraining Loss: 0.572120 \tValidation Loss: 1.042186 \tAccuracy: 0.565934\n",
      "Epoch: 196 \tTraining Loss: 0.569711 \tValidation Loss: 1.047331 \tAccuracy: 0.565934\n",
      "Epoch: 197 \tTraining Loss: 0.568204 \tValidation Loss: 1.047386 \tAccuracy: 0.560440\n",
      "Epoch: 198 \tTraining Loss: 0.565499 \tValidation Loss: 1.050590 \tAccuracy: 0.565934\n",
      "Epoch: 199 \tTraining Loss: 0.563969 \tValidation Loss: 1.050340 \tAccuracy: 0.565934\n",
      "Epoch: 200 \tTraining Loss: 0.560909 \tValidation Loss: 1.052879 \tAccuracy: 0.576923\n",
      "Epoch: 201 \tTraining Loss: 0.558997 \tValidation Loss: 1.056354 \tAccuracy: 0.571429\n",
      "Epoch: 202 \tTraining Loss: 0.556868 \tValidation Loss: 1.056139 \tAccuracy: 0.576923\n",
      "Epoch: 203 \tTraining Loss: 0.554277 \tValidation Loss: 1.059912 \tAccuracy: 0.571429\n",
      "Epoch: 204 \tTraining Loss: 0.552514 \tValidation Loss: 1.061227 \tAccuracy: 0.571429\n",
      "Epoch: 205 \tTraining Loss: 0.550241 \tValidation Loss: 1.062913 \tAccuracy: 0.571429\n",
      "Epoch: 206 \tTraining Loss: 0.548522 \tValidation Loss: 1.061811 \tAccuracy: 0.576923\n",
      "Epoch: 207 \tTraining Loss: 0.546603 \tValidation Loss: 1.064949 \tAccuracy: 0.576923\n",
      "Epoch: 208 \tTraining Loss: 0.544403 \tValidation Loss: 1.065739 \tAccuracy: 0.582418\n",
      "Epoch: 209 \tTraining Loss: 0.542446 \tValidation Loss: 1.067116 \tAccuracy: 0.576923\n",
      "Epoch: 210 \tTraining Loss: 0.540638 \tValidation Loss: 1.068792 \tAccuracy: 0.576923\n",
      "Epoch: 211 \tTraining Loss: 0.538992 \tValidation Loss: 1.069537 \tAccuracy: 0.576923\n",
      "Epoch: 212 \tTraining Loss: 0.536881 \tValidation Loss: 1.068203 \tAccuracy: 0.582418\n",
      "Epoch: 213 \tTraining Loss: 0.535853 \tValidation Loss: 1.067969 \tAccuracy: 0.576923\n",
      "Epoch: 214 \tTraining Loss: 0.533968 \tValidation Loss: 1.068878 \tAccuracy: 0.576923\n",
      "Epoch: 215 \tTraining Loss: 0.532503 \tValidation Loss: 1.069993 \tAccuracy: 0.576923\n",
      "Epoch: 216 \tTraining Loss: 0.531326 \tValidation Loss: 1.069752 \tAccuracy: 0.576923\n",
      "Epoch: 217 \tTraining Loss: 0.530045 \tValidation Loss: 1.070169 \tAccuracy: 0.576923\n",
      "Epoch: 218 \tTraining Loss: 0.529038 \tValidation Loss: 1.070363 \tAccuracy: 0.576923\n",
      "Epoch: 219 \tTraining Loss: 0.528132 \tValidation Loss: 1.070863 \tAccuracy: 0.576923\n",
      "Epoch: 220 \tTraining Loss: 0.527388 \tValidation Loss: 1.071161 \tAccuracy: 0.576923\n",
      "Epoch: 221 \tTraining Loss: 0.526710 \tValidation Loss: 1.071314 \tAccuracy: 0.576923\n",
      "Epoch: 222 \tTraining Loss: 0.526271 \tValidation Loss: 1.071406 \tAccuracy: 0.576923\n",
      "Epoch: 223 \tTraining Loss: 0.525878 \tValidation Loss: 1.071869 \tAccuracy: 0.576923\n",
      "Epoch: 224 \tTraining Loss: 0.525655 \tValidation Loss: 1.072230 \tAccuracy: 0.576923\n",
      "Epoch: 225 \tTraining Loss: 0.525508 \tValidation Loss: 1.072664 \tAccuracy: 0.576923\n",
      "Epoch: 226 \tTraining Loss: 0.525465 \tValidation Loss: 1.073239 \tAccuracy: 0.576923\n",
      "Epoch: 227 \tTraining Loss: 0.525597 \tValidation Loss: 1.073357 \tAccuracy: 0.576923\n",
      "Epoch: 228 \tTraining Loss: 0.525763 \tValidation Loss: 1.074664 \tAccuracy: 0.576923\n",
      "Epoch: 229 \tTraining Loss: 0.526079 \tValidation Loss: 1.075038 \tAccuracy: 0.576923\n",
      "Epoch: 230 \tTraining Loss: 0.526408 \tValidation Loss: 1.075688 \tAccuracy: 0.582418\n",
      "Epoch: 231 \tTraining Loss: 0.526781 \tValidation Loss: 1.076861 \tAccuracy: 0.576923\n",
      "Epoch: 232 \tTraining Loss: 0.527241 \tValidation Loss: 1.077848 \tAccuracy: 0.576923\n",
      "Epoch: 233 \tTraining Loss: 0.527902 \tValidation Loss: 1.078427 \tAccuracy: 0.576923\n",
      "Epoch: 234 \tTraining Loss: 0.528249 \tValidation Loss: 1.080991 \tAccuracy: 0.576923\n",
      "Epoch: 235 \tTraining Loss: 0.528961 \tValidation Loss: 1.081851 \tAccuracy: 0.576923\n",
      "Epoch: 236 \tTraining Loss: 0.529537 \tValidation Loss: 1.084468 \tAccuracy: 0.571429\n",
      "Epoch: 237 \tTraining Loss: 0.529774 \tValidation Loss: 1.085949 \tAccuracy: 0.576923\n",
      "Epoch: 238 \tTraining Loss: 0.530661 \tValidation Loss: 1.088774 \tAccuracy: 0.571429\n",
      "Epoch: 239 \tTraining Loss: 0.531269 \tValidation Loss: 1.091037 \tAccuracy: 0.576923\n",
      "Epoch: 240 \tTraining Loss: 0.531521 \tValidation Loss: 1.090750 \tAccuracy: 0.576923\n",
      "Epoch: 241 \tTraining Loss: 0.531680 \tValidation Loss: 1.095379 \tAccuracy: 0.576923\n",
      "Epoch: 242 \tTraining Loss: 0.532590 \tValidation Loss: 1.098255 \tAccuracy: 0.571429\n",
      "Epoch: 243 \tTraining Loss: 0.532806 \tValidation Loss: 1.099505 \tAccuracy: 0.576923\n",
      "Epoch: 244 \tTraining Loss: 0.532719 \tValidation Loss: 1.103272 \tAccuracy: 0.571429\n",
      "Epoch: 245 \tTraining Loss: 0.533356 \tValidation Loss: 1.106679 \tAccuracy: 0.576923\n",
      "Epoch: 246 \tTraining Loss: 0.533616 \tValidation Loss: 1.109618 \tAccuracy: 0.571429\n",
      "Epoch: 247 \tTraining Loss: 0.532628 \tValidation Loss: 1.110760 \tAccuracy: 0.576923\n",
      "Epoch: 248 \tTraining Loss: 0.533853 \tValidation Loss: 1.116432 \tAccuracy: 0.571429\n",
      "Epoch: 249 \tTraining Loss: 0.532988 \tValidation Loss: 1.116953 \tAccuracy: 0.565934\n",
      "Epoch: 0 \tTraining Loss: 1.036702 \tValidation Loss: 1.041692 \tAccuracy: 0.467033\n",
      "Better loss found. (inf --> 1.041692).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 0.992959 \tValidation Loss: 1.021836 \tAccuracy: 0.478022\n",
      "Better loss found. (1.041692 --> 1.021836).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.967319 \tValidation Loss: 1.007250 \tAccuracy: 0.549451\n",
      "Better loss found. (1.021836 --> 1.007250).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.946587 \tValidation Loss: 0.994858 \tAccuracy: 0.587912\n",
      "Better loss found. (1.007250 --> 0.994858).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.928857 \tValidation Loss: 0.984188 \tAccuracy: 0.576923\n",
      "Better loss found. (0.994858 --> 0.984188).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.913560 \tValidation Loss: 0.975012 \tAccuracy: 0.576923\n",
      "Better loss found. (0.984188 --> 0.975012).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.900331 \tValidation Loss: 0.967124 \tAccuracy: 0.576923\n",
      "Better loss found. (0.975012 --> 0.967124).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.888867 \tValidation Loss: 0.960341 \tAccuracy: 0.576923\n",
      "Better loss found. (0.967124 --> 0.960341).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.878914 \tValidation Loss: 0.954500 \tAccuracy: 0.571429\n",
      "Better loss found. (0.960341 --> 0.954500).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.870255 \tValidation Loss: 0.949462 \tAccuracy: 0.576923\n",
      "Better loss found. (0.954500 --> 0.949462).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.862706 \tValidation Loss: 0.945110 \tAccuracy: 0.576923\n",
      "Better loss found. (0.949462 --> 0.945110).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.856114 \tValidation Loss: 0.941345 \tAccuracy: 0.576923\n",
      "Better loss found. (0.945110 --> 0.941345).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.850348 \tValidation Loss: 0.938084 \tAccuracy: 0.582418\n",
      "Better loss found. (0.941345 --> 0.938084).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.845301 \tValidation Loss: 0.935257 \tAccuracy: 0.576923\n",
      "Better loss found. (0.938084 --> 0.935257).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.840879 \tValidation Loss: 0.932805 \tAccuracy: 0.576923\n",
      "Better loss found. (0.935257 --> 0.932805).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.837003 \tValidation Loss: 0.930677 \tAccuracy: 0.576923\n",
      "Better loss found. (0.932805 --> 0.930677).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.833607 \tValidation Loss: 0.928831 \tAccuracy: 0.576923\n",
      "Better loss found. (0.930677 --> 0.928831).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.830634 \tValidation Loss: 0.927230 \tAccuracy: 0.576923\n",
      "Better loss found. (0.928831 --> 0.927230).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.828033 \tValidation Loss: 0.925844 \tAccuracy: 0.576923\n",
      "Better loss found. (0.927230 --> 0.925844).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.825763 \tValidation Loss: 0.924646 \tAccuracy: 0.576923\n",
      "Better loss found. (0.925844 --> 0.924646).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.823786 \tValidation Loss: 0.923612 \tAccuracy: 0.571429\n",
      "Better loss found. (0.924646 --> 0.923612).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.822068 \tValidation Loss: 0.922723 \tAccuracy: 0.576923\n",
      "Better loss found. (0.923612 --> 0.922723).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.820581 \tValidation Loss: 0.921959 \tAccuracy: 0.576923\n",
      "Better loss found. (0.922723 --> 0.921959).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.819296 \tValidation Loss: 0.921305 \tAccuracy: 0.576923\n",
      "Better loss found. (0.921959 --> 0.921305).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.818192 \tValidation Loss: 0.920747 \tAccuracy: 0.576923\n",
      "Better loss found. (0.921305 --> 0.920747).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.817244 \tValidation Loss: 0.920271 \tAccuracy: 0.576923\n",
      "Better loss found. (0.920747 --> 0.920271).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.816432 \tValidation Loss: 0.919865 \tAccuracy: 0.576923\n",
      "Better loss found. (0.920271 --> 0.919865).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.815736 \tValidation Loss: 0.919518 \tAccuracy: 0.576923\n",
      "Better loss found. (0.919865 --> 0.919518).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.815136 \tValidation Loss: 0.919217 \tAccuracy: 0.576923\n",
      "Better loss found. (0.919518 --> 0.919217).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.814615 \tValidation Loss: 0.918954 \tAccuracy: 0.576923\n",
      "Better loss found. (0.919217 --> 0.918954).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.814154 \tValidation Loss: 0.918717 \tAccuracy: 0.576923\n",
      "Better loss found. (0.918954 --> 0.918717).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.813736 \tValidation Loss: 0.918497 \tAccuracy: 0.576923\n",
      "Better loss found. (0.918717 --> 0.918497).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.813342 \tValidation Loss: 0.918284 \tAccuracy: 0.576923\n",
      "Better loss found. (0.918497 --> 0.918284).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.812956 \tValidation Loss: 0.918069 \tAccuracy: 0.576923\n",
      "Better loss found. (0.918284 --> 0.918069).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.812562 \tValidation Loss: 0.917844 \tAccuracy: 0.576923\n",
      "Better loss found. (0.918069 --> 0.917844).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.812144 \tValidation Loss: 0.917599 \tAccuracy: 0.576923\n",
      "Better loss found. (0.917844 --> 0.917599).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.811688 \tValidation Loss: 0.917329 \tAccuracy: 0.576923\n",
      "Better loss found. (0.917599 --> 0.917329).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.811179 \tValidation Loss: 0.917027 \tAccuracy: 0.576923\n",
      "Better loss found. (0.917329 --> 0.917027).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.810606 \tValidation Loss: 0.916687 \tAccuracy: 0.576923\n",
      "Better loss found. (0.917027 --> 0.916687).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.809959 \tValidation Loss: 0.916305 \tAccuracy: 0.576923\n",
      "Better loss found. (0.916687 --> 0.916305).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.809230 \tValidation Loss: 0.915879 \tAccuracy: 0.576923\n",
      "Better loss found. (0.916305 --> 0.915879).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.808413 \tValidation Loss: 0.915408 \tAccuracy: 0.576923\n",
      "Better loss found. (0.915879 --> 0.915408).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.807504 \tValidation Loss: 0.914891 \tAccuracy: 0.576923\n",
      "Better loss found. (0.915408 --> 0.914891).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.806503 \tValidation Loss: 0.914329 \tAccuracy: 0.576923\n",
      "Better loss found. (0.914891 --> 0.914329).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.805412 \tValidation Loss: 0.913726 \tAccuracy: 0.576923\n",
      "Better loss found. (0.914329 --> 0.913726).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.804235 \tValidation Loss: 0.913086 \tAccuracy: 0.576923\n",
      "Better loss found. (0.913726 --> 0.913086).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.802977 \tValidation Loss: 0.912413 \tAccuracy: 0.576923\n",
      "Better loss found. (0.913086 --> 0.912413).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.801649 \tValidation Loss: 0.911714 \tAccuracy: 0.576923\n",
      "Better loss found. (0.912413 --> 0.911714).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.800258 \tValidation Loss: 0.910994 \tAccuracy: 0.576923\n",
      "Better loss found. (0.911714 --> 0.910994).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.798817 \tValidation Loss: 0.910262 \tAccuracy: 0.576923\n",
      "Better loss found. (0.910994 --> 0.910262).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.797338 \tValidation Loss: 0.909523 \tAccuracy: 0.582418\n",
      "Better loss found. (0.910262 --> 0.909523).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.795833 \tValidation Loss: 0.908786 \tAccuracy: 0.582418\n",
      "Better loss found. (0.909523 --> 0.908786).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.794313 \tValidation Loss: 0.908057 \tAccuracy: 0.582418\n",
      "Better loss found. (0.908786 --> 0.908057).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.792792 \tValidation Loss: 0.907342 \tAccuracy: 0.582418\n",
      "Better loss found. (0.908057 --> 0.907342).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.791280 \tValidation Loss: 0.906647 \tAccuracy: 0.582418\n",
      "Better loss found. (0.907342 --> 0.906647).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.789788 \tValidation Loss: 0.905977 \tAccuracy: 0.582418\n",
      "Better loss found. (0.906647 --> 0.905977).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.788325 \tValidation Loss: 0.905335 \tAccuracy: 0.582418\n",
      "Better loss found. (0.905977 --> 0.905335).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.786899 \tValidation Loss: 0.904724 \tAccuracy: 0.582418\n",
      "Better loss found. (0.905335 --> 0.904724).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.785516 \tValidation Loss: 0.904148 \tAccuracy: 0.582418\n",
      "Better loss found. (0.904724 --> 0.904148).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.784182 \tValidation Loss: 0.903606 \tAccuracy: 0.582418\n",
      "Better loss found. (0.904148 --> 0.903606).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.782902 \tValidation Loss: 0.903101 \tAccuracy: 0.587912\n",
      "Better loss found. (0.903606 --> 0.903101).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.781679 \tValidation Loss: 0.902631 \tAccuracy: 0.587912\n",
      "Better loss found. (0.903101 --> 0.902631).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.780514 \tValidation Loss: 0.902197 \tAccuracy: 0.587912\n",
      "Better loss found. (0.902631 --> 0.902197).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.779409 \tValidation Loss: 0.901797 \tAccuracy: 0.587912\n",
      "Better loss found. (0.902197 --> 0.901797).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.778364 \tValidation Loss: 0.901430 \tAccuracy: 0.587912\n",
      "Better loss found. (0.901797 --> 0.901430).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.777379 \tValidation Loss: 0.901095 \tAccuracy: 0.587912\n",
      "Better loss found. (0.901430 --> 0.901095).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.776452 \tValidation Loss: 0.900790 \tAccuracy: 0.587912\n",
      "Better loss found. (0.901095 --> 0.900790).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.775583 \tValidation Loss: 0.900512 \tAccuracy: 0.582418\n",
      "Better loss found. (0.900790 --> 0.900512).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.774770 \tValidation Loss: 0.900260 \tAccuracy: 0.582418\n",
      "Better loss found. (0.900512 --> 0.900260).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.774010 \tValidation Loss: 0.900032 \tAccuracy: 0.582418\n",
      "Better loss found. (0.900260 --> 0.900032).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.773302 \tValidation Loss: 0.899826 \tAccuracy: 0.582418\n",
      "Better loss found. (0.900032 --> 0.899826).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.772643 \tValidation Loss: 0.899641 \tAccuracy: 0.587912\n",
      "Better loss found. (0.899826 --> 0.899641).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.772031 \tValidation Loss: 0.899473 \tAccuracy: 0.587912\n",
      "Better loss found. (0.899641 --> 0.899473).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.771463 \tValidation Loss: 0.899323 \tAccuracy: 0.587912\n",
      "Better loss found. (0.899473 --> 0.899323).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.770937 \tValidation Loss: 0.899187 \tAccuracy: 0.582418\n",
      "Better loss found. (0.899323 --> 0.899187).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.770451 \tValidation Loss: 0.899066 \tAccuracy: 0.582418\n",
      "Better loss found. (0.899187 --> 0.899066).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.770003 \tValidation Loss: 0.898957 \tAccuracy: 0.582418\n",
      "Better loss found. (0.899066 --> 0.898957).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.769591 \tValidation Loss: 0.898859 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898957 --> 0.898859).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.769212 \tValidation Loss: 0.898771 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898859 --> 0.898771).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.768865 \tValidation Loss: 0.898693 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898771 --> 0.898693).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.768548 \tValidation Loss: 0.898624 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898693 --> 0.898624).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.768259 \tValidation Loss: 0.898562 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898624 --> 0.898562).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.767997 \tValidation Loss: 0.898507 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898562 --> 0.898507).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.767760 \tValidation Loss: 0.898458 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898507 --> 0.898458).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.767547 \tValidation Loss: 0.898416 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898458 --> 0.898416).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.767356 \tValidation Loss: 0.898378 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898416 --> 0.898378).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.767186 \tValidation Loss: 0.898345 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898378 --> 0.898345).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.767035 \tValidation Loss: 0.898317 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898345 --> 0.898317).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.766902 \tValidation Loss: 0.898292 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898317 --> 0.898292).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.766786 \tValidation Loss: 0.898271 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898292 --> 0.898271).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.766685 \tValidation Loss: 0.898252 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898271 --> 0.898252).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.766598 \tValidation Loss: 0.898236 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898252 --> 0.898236).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.766524 \tValidation Loss: 0.898222 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898236 --> 0.898222).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.766460 \tValidation Loss: 0.898210 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898222 --> 0.898210).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.766405 \tValidation Loss: 0.898199 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898210 --> 0.898199).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.766358 \tValidation Loss: 0.898189 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898199 --> 0.898189).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.766317 \tValidation Loss: 0.898180 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898189 --> 0.898180).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.766280 \tValidation Loss: 0.898171 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898180 --> 0.898171).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.766246 \tValidation Loss: 0.898161 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898171 --> 0.898161).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.766213 \tValidation Loss: 0.898151 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898161 --> 0.898151).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.766179 \tValidation Loss: 0.898140 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898151 --> 0.898140).  Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 0.766143 \tValidation Loss: 0.898129 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898140 --> 0.898129).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.766103 \tValidation Loss: 0.898117 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898129 --> 0.898117).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 0.766059 \tValidation Loss: 0.898105 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898117 --> 0.898105).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 0.766008 \tValidation Loss: 0.898092 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898105 --> 0.898092).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 0.765950 \tValidation Loss: 0.898080 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898092 --> 0.898080).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.765885 \tValidation Loss: 0.898067 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898080 --> 0.898067).  Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 0.765810 \tValidation Loss: 0.898055 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898067 --> 0.898055).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 0.765726 \tValidation Loss: 0.898044 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898055 --> 0.898044).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 0.765633 \tValidation Loss: 0.898033 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898044 --> 0.898033).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 0.765530 \tValidation Loss: 0.898023 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898033 --> 0.898023).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 0.765417 \tValidation Loss: 0.898013 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898023 --> 0.898013).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.765295 \tValidation Loss: 0.898005 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898013 --> 0.898005).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 0.765164 \tValidation Loss: 0.897997 \tAccuracy: 0.582418\n",
      "Better loss found. (0.898005 --> 0.897997).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 0.765024 \tValidation Loss: 0.897989 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897997 --> 0.897989).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 0.764877 \tValidation Loss: 0.897982 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897989 --> 0.897982).  Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 0.764722 \tValidation Loss: 0.897976 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897982 --> 0.897976).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.764562 \tValidation Loss: 0.897971 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897976 --> 0.897971).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 0.764396 \tValidation Loss: 0.897966 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897971 --> 0.897966).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 0.764226 \tValidation Loss: 0.897962 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897966 --> 0.897962).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 0.764052 \tValidation Loss: 0.897959 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897962 --> 0.897959).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 0.763877 \tValidation Loss: 0.897956 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897959 --> 0.897956).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 0.763699 \tValidation Loss: 0.897954 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897956 --> 0.897954).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.763521 \tValidation Loss: 0.897952 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897954 --> 0.897952).  Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 0.763343 \tValidation Loss: 0.897951 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897952 --> 0.897951).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 0.763166 \tValidation Loss: 0.897950 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897951 --> 0.897950).  Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 0.762991 \tValidation Loss: 0.897950 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897950 --> 0.897950).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.762817 \tValidation Loss: 0.897949 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897950 --> 0.897949).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.762646 \tValidation Loss: 0.897948 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897949 --> 0.897948).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.762477 \tValidation Loss: 0.897947 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897948 --> 0.897947).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.762312 \tValidation Loss: 0.897945 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897947 --> 0.897945).  Saving model ...\n",
      "Epoch: 131 \tTraining Loss: 0.762150 \tValidation Loss: 0.897943 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897945 --> 0.897943).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 0.761992 \tValidation Loss: 0.897940 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897943 --> 0.897940).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 0.761838 \tValidation Loss: 0.897937 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897940 --> 0.897937).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 0.761688 \tValidation Loss: 0.897933 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897937 --> 0.897933).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.761543 \tValidation Loss: 0.897929 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897933 --> 0.897929).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 0.761402 \tValidation Loss: 0.897924 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897929 --> 0.897924).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 0.761265 \tValidation Loss: 0.897918 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897924 --> 0.897918).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 0.761133 \tValidation Loss: 0.897912 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897918 --> 0.897912).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 0.761006 \tValidation Loss: 0.897905 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897912 --> 0.897905).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 0.760884 \tValidation Loss: 0.897898 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897905 --> 0.897898).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 0.760766 \tValidation Loss: 0.897891 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897898 --> 0.897891).  Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 0.760654 \tValidation Loss: 0.897883 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897891 --> 0.897883).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.760548 \tValidation Loss: 0.897876 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897883 --> 0.897876).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 0.760447 \tValidation Loss: 0.897868 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897876 --> 0.897868).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 0.760351 \tValidation Loss: 0.897861 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897868 --> 0.897861).  Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 0.760261 \tValidation Loss: 0.897854 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897861 --> 0.897854).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 0.760177 \tValidation Loss: 0.897847 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897854 --> 0.897847).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 0.760099 \tValidation Loss: 0.897840 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897847 --> 0.897840).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 0.760026 \tValidation Loss: 0.897834 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897840 --> 0.897834).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 0.759960 \tValidation Loss: 0.897828 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897834 --> 0.897828).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 0.759900 \tValidation Loss: 0.897823 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897828 --> 0.897823).  Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 0.759846 \tValidation Loss: 0.897818 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897823 --> 0.897818).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 0.759797 \tValidation Loss: 0.897814 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897818 --> 0.897814).  Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 0.759755 \tValidation Loss: 0.897810 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897814 --> 0.897810).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 0.759718 \tValidation Loss: 0.897807 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897810 --> 0.897807).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 0.759687 \tValidation Loss: 0.897804 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897807 --> 0.897804).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 0.759661 \tValidation Loss: 0.897801 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897804 --> 0.897801).  Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 0.759641 \tValidation Loss: 0.897799 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897801 --> 0.897799).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 0.759625 \tValidation Loss: 0.897797 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897799 --> 0.897797).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.759613 \tValidation Loss: 0.897795 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897797 --> 0.897795).  Saving model ...\n",
      "Epoch: 161 \tTraining Loss: 0.759606 \tValidation Loss: 0.897794 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897795 --> 0.897794).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 0.759602 \tValidation Loss: 0.897792 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897794 --> 0.897792).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.759602 \tValidation Loss: 0.897791 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897792 --> 0.897791).  Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 0.759604 \tValidation Loss: 0.897790 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897791 --> 0.897790).  Saving model ...\n",
      "Epoch: 165 \tTraining Loss: 0.759609 \tValidation Loss: 0.897790 \tAccuracy: 0.582418\n",
      "Better loss found. (0.897790 --> 0.897790).  Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 0.759615 \tValidation Loss: 0.897790 \tAccuracy: 0.582418\n",
      "Epoch: 167 \tTraining Loss: 0.759623 \tValidation Loss: 0.897792 \tAccuracy: 0.582418\n",
      "Epoch: 168 \tTraining Loss: 0.759632 \tValidation Loss: 0.897795 \tAccuracy: 0.582418\n",
      "Epoch: 169 \tTraining Loss: 0.759642 \tValidation Loss: 0.897799 \tAccuracy: 0.582418\n",
      "Epoch: 170 \tTraining Loss: 0.759651 \tValidation Loss: 0.897806 \tAccuracy: 0.582418\n",
      "Epoch: 171 \tTraining Loss: 0.759661 \tValidation Loss: 0.897814 \tAccuracy: 0.582418\n",
      "Epoch: 172 \tTraining Loss: 0.759670 \tValidation Loss: 0.897824 \tAccuracy: 0.582418\n",
      "Epoch: 173 \tTraining Loss: 0.759677 \tValidation Loss: 0.897837 \tAccuracy: 0.582418\n",
      "Epoch: 174 \tTraining Loss: 0.759684 \tValidation Loss: 0.897852 \tAccuracy: 0.582418\n",
      "Epoch: 175 \tTraining Loss: 0.759689 \tValidation Loss: 0.897868 \tAccuracy: 0.582418\n",
      "Epoch: 176 \tTraining Loss: 0.759691 \tValidation Loss: 0.897886 \tAccuracy: 0.582418\n",
      "Epoch: 177 \tTraining Loss: 0.759691 \tValidation Loss: 0.897905 \tAccuracy: 0.582418\n",
      "Epoch: 178 \tTraining Loss: 0.759689 \tValidation Loss: 0.897926 \tAccuracy: 0.582418\n",
      "Epoch: 179 \tTraining Loss: 0.759683 \tValidation Loss: 0.897947 \tAccuracy: 0.582418\n",
      "Epoch: 180 \tTraining Loss: 0.759675 \tValidation Loss: 0.897968 \tAccuracy: 0.582418\n",
      "Epoch: 181 \tTraining Loss: 0.759663 \tValidation Loss: 0.897989 \tAccuracy: 0.582418\n",
      "Epoch: 182 \tTraining Loss: 0.759648 \tValidation Loss: 0.898011 \tAccuracy: 0.582418\n",
      "Epoch: 183 \tTraining Loss: 0.759630 \tValidation Loss: 0.898032 \tAccuracy: 0.582418\n",
      "Epoch: 184 \tTraining Loss: 0.759608 \tValidation Loss: 0.898052 \tAccuracy: 0.582418\n",
      "Epoch: 185 \tTraining Loss: 0.759583 \tValidation Loss: 0.898072 \tAccuracy: 0.582418\n",
      "Epoch: 186 \tTraining Loss: 0.759554 \tValidation Loss: 0.898091 \tAccuracy: 0.582418\n",
      "Epoch: 187 \tTraining Loss: 0.759522 \tValidation Loss: 0.898109 \tAccuracy: 0.582418\n",
      "Epoch: 188 \tTraining Loss: 0.759486 \tValidation Loss: 0.898125 \tAccuracy: 0.582418\n",
      "Epoch: 189 \tTraining Loss: 0.759447 \tValidation Loss: 0.898140 \tAccuracy: 0.582418\n",
      "Epoch: 190 \tTraining Loss: 0.759405 \tValidation Loss: 0.898153 \tAccuracy: 0.582418\n",
      "Epoch: 191 \tTraining Loss: 0.759360 \tValidation Loss: 0.898165 \tAccuracy: 0.582418\n",
      "Epoch: 192 \tTraining Loss: 0.759312 \tValidation Loss: 0.898175 \tAccuracy: 0.582418\n",
      "Epoch: 193 \tTraining Loss: 0.759261 \tValidation Loss: 0.898183 \tAccuracy: 0.582418\n",
      "Epoch: 194 \tTraining Loss: 0.759208 \tValidation Loss: 0.898190 \tAccuracy: 0.582418\n",
      "Epoch: 195 \tTraining Loss: 0.759152 \tValidation Loss: 0.898195 \tAccuracy: 0.582418\n",
      "Epoch: 196 \tTraining Loss: 0.759094 \tValidation Loss: 0.898198 \tAccuracy: 0.582418\n",
      "Epoch: 197 \tTraining Loss: 0.759034 \tValidation Loss: 0.898199 \tAccuracy: 0.576923\n",
      "Epoch: 198 \tTraining Loss: 0.758972 \tValidation Loss: 0.898199 \tAccuracy: 0.576923\n",
      "Epoch: 199 \tTraining Loss: 0.758908 \tValidation Loss: 0.898197 \tAccuracy: 0.576923\n",
      "Epoch: 200 \tTraining Loss: 0.758843 \tValidation Loss: 0.898194 \tAccuracy: 0.576923\n",
      "Epoch: 201 \tTraining Loss: 0.758778 \tValidation Loss: 0.898190 \tAccuracy: 0.582418\n",
      "Epoch: 202 \tTraining Loss: 0.758711 \tValidation Loss: 0.898185 \tAccuracy: 0.582418\n",
      "Epoch: 203 \tTraining Loss: 0.758645 \tValidation Loss: 0.898179 \tAccuracy: 0.582418\n",
      "Epoch: 204 \tTraining Loss: 0.758578 \tValidation Loss: 0.898172 \tAccuracy: 0.582418\n",
      "Epoch: 205 \tTraining Loss: 0.758512 \tValidation Loss: 0.898165 \tAccuracy: 0.582418\n",
      "Epoch: 206 \tTraining Loss: 0.758446 \tValidation Loss: 0.898157 \tAccuracy: 0.582418\n",
      "Epoch: 207 \tTraining Loss: 0.758382 \tValidation Loss: 0.898149 \tAccuracy: 0.582418\n",
      "Epoch: 208 \tTraining Loss: 0.758319 \tValidation Loss: 0.898142 \tAccuracy: 0.582418\n",
      "Epoch: 209 \tTraining Loss: 0.758258 \tValidation Loss: 0.898134 \tAccuracy: 0.582418\n",
      "Epoch: 210 \tTraining Loss: 0.758199 \tValidation Loss: 0.898126 \tAccuracy: 0.582418\n",
      "Epoch: 211 \tTraining Loss: 0.758142 \tValidation Loss: 0.898118 \tAccuracy: 0.582418\n",
      "Epoch: 212 \tTraining Loss: 0.758088 \tValidation Loss: 0.898111 \tAccuracy: 0.582418\n",
      "Epoch: 213 \tTraining Loss: 0.758038 \tValidation Loss: 0.898105 \tAccuracy: 0.582418\n",
      "Epoch: 214 \tTraining Loss: 0.757990 \tValidation Loss: 0.898098 \tAccuracy: 0.582418\n",
      "Epoch: 215 \tTraining Loss: 0.757947 \tValidation Loss: 0.898093 \tAccuracy: 0.582418\n",
      "Epoch: 216 \tTraining Loss: 0.757907 \tValidation Loss: 0.898087 \tAccuracy: 0.582418\n",
      "Epoch: 217 \tTraining Loss: 0.757871 \tValidation Loss: 0.898083 \tAccuracy: 0.582418\n",
      "Epoch: 218 \tTraining Loss: 0.757839 \tValidation Loss: 0.898079 \tAccuracy: 0.582418\n",
      "Epoch: 219 \tTraining Loss: 0.757812 \tValidation Loss: 0.898075 \tAccuracy: 0.582418\n",
      "Epoch: 220 \tTraining Loss: 0.757789 \tValidation Loss: 0.898072 \tAccuracy: 0.582418\n",
      "Epoch: 221 \tTraining Loss: 0.757770 \tValidation Loss: 0.898069 \tAccuracy: 0.582418\n",
      "Epoch: 222 \tTraining Loss: 0.757756 \tValidation Loss: 0.898066 \tAccuracy: 0.582418\n",
      "Epoch: 223 \tTraining Loss: 0.757746 \tValidation Loss: 0.898064 \tAccuracy: 0.582418\n",
      "Epoch: 224 \tTraining Loss: 0.757740 \tValidation Loss: 0.898062 \tAccuracy: 0.582418\n",
      "Epoch: 225 \tTraining Loss: 0.757738 \tValidation Loss: 0.898060 \tAccuracy: 0.582418\n",
      "Epoch: 226 \tTraining Loss: 0.757740 \tValidation Loss: 0.898058 \tAccuracy: 0.582418\n",
      "Epoch: 227 \tTraining Loss: 0.757746 \tValidation Loss: 0.898057 \tAccuracy: 0.582418\n",
      "Epoch: 228 \tTraining Loss: 0.757755 \tValidation Loss: 0.898056 \tAccuracy: 0.582418\n",
      "Epoch: 229 \tTraining Loss: 0.757767 \tValidation Loss: 0.898055 \tAccuracy: 0.582418\n",
      "Epoch: 230 \tTraining Loss: 0.757783 \tValidation Loss: 0.898055 \tAccuracy: 0.582418\n",
      "Epoch: 231 \tTraining Loss: 0.757801 \tValidation Loss: 0.898056 \tAccuracy: 0.582418\n",
      "Epoch: 232 \tTraining Loss: 0.757821 \tValidation Loss: 0.898058 \tAccuracy: 0.582418\n",
      "Epoch: 233 \tTraining Loss: 0.757844 \tValidation Loss: 0.898061 \tAccuracy: 0.582418\n",
      "Epoch: 234 \tTraining Loss: 0.757868 \tValidation Loss: 0.898066 \tAccuracy: 0.582418\n",
      "Epoch: 235 \tTraining Loss: 0.757893 \tValidation Loss: 0.898073 \tAccuracy: 0.582418\n",
      "Epoch: 236 \tTraining Loss: 0.757920 \tValidation Loss: 0.898082 \tAccuracy: 0.582418\n",
      "Epoch: 237 \tTraining Loss: 0.757948 \tValidation Loss: 0.898093 \tAccuracy: 0.582418\n",
      "Epoch: 238 \tTraining Loss: 0.757975 \tValidation Loss: 0.898106 \tAccuracy: 0.582418\n",
      "Epoch: 239 \tTraining Loss: 0.758003 \tValidation Loss: 0.898120 \tAccuracy: 0.582418\n",
      "Epoch: 240 \tTraining Loss: 0.758030 \tValidation Loss: 0.898136 \tAccuracy: 0.582418\n",
      "Epoch: 241 \tTraining Loss: 0.758056 \tValidation Loss: 0.898152 \tAccuracy: 0.582418\n",
      "Epoch: 242 \tTraining Loss: 0.758081 \tValidation Loss: 0.898170 \tAccuracy: 0.582418\n",
      "Epoch: 243 \tTraining Loss: 0.758104 \tValidation Loss: 0.898188 \tAccuracy: 0.582418\n",
      "Epoch: 244 \tTraining Loss: 0.758126 \tValidation Loss: 0.898206 \tAccuracy: 0.582418\n",
      "Epoch: 245 \tTraining Loss: 0.758144 \tValidation Loss: 0.898224 \tAccuracy: 0.582418\n",
      "Epoch: 246 \tTraining Loss: 0.758160 \tValidation Loss: 0.898242 \tAccuracy: 0.582418\n",
      "Epoch: 247 \tTraining Loss: 0.758173 \tValidation Loss: 0.898259 \tAccuracy: 0.582418\n",
      "Epoch: 248 \tTraining Loss: 0.758183 \tValidation Loss: 0.898276 \tAccuracy: 0.582418\n",
      "Epoch: 249 \tTraining Loss: 0.758190 \tValidation Loss: 0.898291 \tAccuracy: 0.582418\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "num_models = 3\n",
    "models = [Net(input_size, hidden_sizes, output_size) for _ in range(num_models)]\n",
    "for i in range(num_models):\n",
    "    models[i] = train_model(models[i], train_data, val_data, num_epochs)\n",
    "ensemble_model = EnsembleModel(output_size, models)\n",
    "model = train_model(ensemble_model, train_data, val_data, num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, con el modelo anterior almacenado en *emotions_by_voice.registers.pt* se puede cargar en modelo model, recuperando el modelo entrenado y usarlo para realizar predicciones en nuevos datos, sin tener que volver a entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_FILENAME))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Observación de los Parámetros del Mejor Modelo.\n",
    "\n",
    "Adicionalmente se imprimen los parámetros entrenados del clasificador lineal del modelo. El tensor classifier.weight contiene las ponderaciones de la capa lineal y classifier.bias contiene los términos de sesgo. En el caso de este modelo, el clasificador tiene tres neuronas de salida, por lo que hay tres conjuntos de pesos y un conjunto de términos de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight tensor([[ 0.9028, -0.2897, -0.9745,  0.8830, -0.7283, -0.7548,  1.2011, -0.3816,\n",
      "         -0.2659],\n",
      "        [-0.5549,  0.9675, -0.2717, -0.3562,  1.2013, -1.0051, -0.3423,  0.4635,\n",
      "         -0.7328],\n",
      "        [-1.0012, -0.7697,  1.0217, -0.3533, -0.6546,  1.5763, -0.5847, -0.6591,\n",
      "          0.9419]])\n",
      "classifier.bias tensor([-0.1576, -0.0560, -0.2373])\n"
     ]
    }
   ],
   "source": [
    "for feature, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(feature, param.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Evaluación del Modelo.\n",
    "\n",
    "Para terminar se evalúa el modelo entrenado anteriormente en el conjunto de datos de prueba. Primero, el modelo se pone en modo de evaluación con *model.eval()*. Luego, para cada lote de datos y etiquetas de destino en el conjunto de datos de prueba, se pasa el lote a través del modelo para obtener las predicciones de salida. La etiqueta predicha es la clase con la salida más alta en el tensor de salida. Se mantiene un seguimiento del número total de predicciones correctas y se almacenan las etiquetas reales y predichas. Finalmente, se calcula la precisión del modelo en el conjunto de datos de prueba y se imprime. Luego se genera una matriz de confusión y se muestra con la ayuda de la biblioteca *matplotlib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.692308\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "y_pred = []\n",
    "y_real = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_data:\n",
    "        output = model(data)\n",
    "        pred = torch.max(output, 1)[1]\n",
    "    \n",
    "        batch_size = target.size(0)\n",
    "        total += batch_size\n",
    "        correct += (pred == target).float().sum().item()\n",
    "    \n",
    "        y_pred.extend(np.array(pred).flatten())\n",
    "        y_real.extend(np.array(target).flatten())\n",
    "\n",
    "\n",
    "acc = correct / total\n",
    "print(\"Accuracy: {:.6f}\".format(acc))\n",
    "\n",
    "y_real = le.inverse_transform(y_real)\n",
    "y_pred = le.inverse_transform(y_pred)\n",
    "cm = confusion_matrix(y_real, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot()\n",
    "plt.xlabel(\"Predicciones\")\n",
    "plt.ylabel(\"Valores reales\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de confusión muestra las predicciones del modelo en relación a las clases reales de los datos de prueba. En este caso, la matriz es de tamaño 3x3 porque el modelo tiene tres clases posibles: **angry**, **happy**, **sad**. Los valores en la diagonal de la matriz representan las predicciones correctas del modelo, mientras que los valores fuera de la diagonal representan las predicciones incorrectas.  En este caso, podemos observar que la clase **angry** fue predicha correctamente 41 veces, pero fue confundida con la clase **happy** en 7 ocasiones y con la clase **sad** en 18 ocasiones. La clase **happy** (segunda fila) fue predicha correctamente 34 veces, pero fue confundida con la clase **angry** en 14 ocasiones y con la clase **sad** en 7 ocasiones. La clase **sad** (tercera fila) fue predicha correctamente 51 veces, pero fue confundida con la clase **angry** en 5 ocasiones y con la clase **happy** en 5 ocasiones. En general, podemos decir que el modelo parece tener un buen desempeño en la clasificación de la clase **sad**, pero tiene más dificultades para distinguir entre las clases **angry** y **happy**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Predicción de Sentimientos Aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La predicción para la muestra elegida es: ['happy' 'sad' 'sad']\n",
      "El valor esperado era:  ['happy' 'sad' 'angry']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "np.random.seed(0)\n",
    "size = 3\n",
    "random_indices = np.random.choice(df.index, size = size, replace = False)\n",
    "new_X = df.drop(columns = ['label']).take(random_indices).values # type: ignore\n",
    "new_Y = df['label'].take(random_indices).values # type: ignore\n",
    "\n",
    "random_values = MyDataset(new_X, new_Y)\n",
    "\n",
    "random_data = DataLoader(\n",
    "    random_values,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "y_pred = []\n",
    "y_real = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in random_data:\n",
    "        outputs = model(data)\n",
    "        pred = outputs.argmax(dim = 1)\n",
    "        y_pred.extend(np.array(pred).flatten())\n",
    "        y_real.extend(np.array(target).flatten())\n",
    "print(\"La predicción para la muestra elegida es:\", le.inverse_transform(y_pred))\n",
    "print(\"El valor esperado era: \", le.inverse_transform(y_real))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Conclusiones.\n",
    "\n",
    "Después de la revisión del código y los resultados obtenidos, podemos concluir lo siguiente:\n",
    "\n",
    "En primer lugar, considerando que se realiza la identificación y posterior imputación de outliers en los datos utilizando la técnica de imputación de valores faltantes K-Vecinos más Cercanos (KNNImputer) y la prueba de valores Z para la detección de outliers, es posible que la calidad de los datos haya mejorado, lo que puede haber contribuido a la mejora del desempeño del modelo.\n",
    "\n",
    "En segundo lugar, el conjunto de datos tiene 909 registros, de los cuales 545 se usaron para entrenar el modelo, 182 para validación y 182 para prueba. Aunque esto puede ser un número razonable de datos para algunos problemas, para otros, como la clasificación de emociones, podría ser necesario tener un conjunto de datos más grande para entrenar un modelo que tenga un mejor desempeño. En general es el tamaño del conjunto de entrenamiento, validación y pruebas es adecuado, y la selección de características utilizando la prueba F también puede haber ayudado a mejorar la eficiencia del modelo.\n",
    "\n",
    "En tercer lugar, la arquitectura de la red neuronal se define mediante la clase Net y EnsembleModel, con una capa de entrada de 17 nodos, dos capas ocultas con 14 y 10 nodos respectivamente, y una capa de salida de 3 nodos. Se entrenaron tres modelos y se combinaron en un modelo de conjunto (EnsembleModel), que se entrenó en los mismos datos. Se utilizaron 100 épocas de entrenamiento y se obtuvo una precisión (accuracy) del 0.692308, razonablemente bueno, pero no suficiente.\n",
    "\n",
    "En cuarto lugar, la matriz de confusión muestra que el modelo tuvo un rendimiento sólido en la clasificación de la clase sad, pero tuvo dificultades para distinguir entre las clases angry y happy. Estos resultados sugieren que puede haber una superposición entre estas dos clases en términos de las características que el modelo está utilizando para hacer sus predicciones.\n",
    "\n",
    "Aunque la precisión obtenida no es muy alta, hay que tener en cuenta que la clasificación de emociones es un problema complejo, especialmente si se considera la gran complejidad de las emociones y las diferentes condiciones en las que pueden ser capturadas. Por lo que, si la base de datos no es representativa o está sesgada, el modelo puede tener dificultades para generalizar y producir resultados precisos. Por lo tanto, es importante tener en cuenta la calidad y representatividad de los datos utilizados en el entrenamiento del modelo. Particularmente, parece ser el problema en esa implementación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
