---
title: "Resumen"
author: "Laura Gonzalez"
date: "2023-03-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aspectos del Analisis Multivariado

```{r} 
knitr::include_graphics("2.png") 
```

```{r}
library(Matrix)

x1 = c(42,52,48,58);
x2 = c(4,5,4,3);

X <- cbind(x1,x2);X
```

## Vector de Medias

You can also embed plots, for example:
```{r}
x_bar <- colMeans(X); x_bar
```

## Matriz de varianzas y covarianzas

```{r}
Sn <- cov(X)*(nrow(X)-1)/nrow(X);Sn
```
## Matriz de correlación

```{r}
R<-cor(X);R

```

## Distancia de x1 a x2

```{r}

dist_1 <- function(x1,x2){
  resu = 0;
  for (i in 1:length(x1)){
    resu = resu + (x1[i]-x2[i])^2;  
  }
  return(sqrt(resu))
}

dist_1(X[,1],X[,2])

```

# Algebra Matricial y vectores aleatrorios

## Transpuesta

Simetrica es lo mismo que transpuesta


```{r}

x = matrix(c(1,2,3),1,3);x
x_com=t(x);x_com
```

## Longitud de x

```{r}

lengt <- function(x){ 
  n = length(x); 
  value = 0; 
  for( i in 1:n){ 
    value = value + (x[i])^2; 
  } 
  return(sqrt(value)) 
}

X = c(1,2,3);
lengt(X)

```
## Angulo entre dos vectores

```{r}
x <- matrix(c(1,3,2),3,1);x
y <- matrix(c(-2,1,-1),3,1);y
cos_theta <- function(x1,x2){
  return((t(x1)%*%x2)/(lengt(x1)*lengt(x2)))
}
cos_theta(x,y)

```

## Vectores Independientes

Revisar si es importantes.

```{r}



```


## Proyección de x en y

```{r}

proy_x_y <- function(x,y){
  return( as.numeric((t(x)%*%y/t(y)%*%y)) * y )
}

proy_x_y(x,y)
```

## Inversa de una matriz


```{r}

A <- rbind(c(2,-2,2),c(2,1,0),c(3,-2,2));A
A_inv <- solve(A);A_inv

```

## Matriz definida positiva
Una matriz es definida positiva, Si todos los autovalores son positivos, la matriz es definida positivamente. Si hay autovalores negativos o cero, la matriz no es definida positivamente.

```{r}

A <- rbind(c(9,-2),c(-2,6));A

eig <- eigen(A);

eig$values

eig$vectors

```
## Descomposición espectral.

```{r}

des_espectral <- function(A){
  ro = nrow(A);  lambda <- eigen(A);
  n = length(lambda);
  Result = 0;
  for (i in 1:n) {
    print(sprintf("El %d valor propio de la matriz es: %g", i, lambda$values[i]))
    vect_pro <- matrix(c(lambda$vectors[,i]), nrow = ro, ncol = 1);
    print(sprintf("El vector propio es %s.", toString(vect_pro)))
    print(sprintf("Obteniendo la matriz %d :", i))
    Opera = lambda$values[i]*vect_pro%*%t(vect_pro);
    print(Opera)
    Result = Result + Opera;
  }
  print("El resultado de sumar las matrices x' es:")
  print(Result)
  return(Result)
}

Z <- des_espectral(A);

```

## Matriz raiz cuadrada


```{r}

A <- rbind(c(2,1),c(1,3));A

root_square <- function(A){
  eig <- eigen(A);
  x  = (eig$vectors) %*% sqrt(diag(as.vector(eig$values)))  %*% t(eig$vectors);
  return(x)
  
}

ra_A <- root_square(A);ra_A
in_ra_a <- solve(ra_A); in_ra_a

round(root_square(A) %*% solve(ra_A))

```

# Geometria de la muestra y muestras aleatorias

## Descomposición en las componentes de desviación

```{r}
dd_dot <- function(x,y){ 
  return(x%*%y) 
}

x <- cbind(c(4,-1,3),c(1,3,5));x 
means <- colMeans(x);means 
x_t = t(x);
d1 = x_t[1,] - means[1]*c(1,1,1);
d2 = x_t[2,] - means[2]*c(1,1,1); 
```

Relacione estas cantidades con $S_n$ y $R$.

Note que $d_x'dy = n \cdot s_{xy}$, donde $n$ es la dimensionalidad de las variables. Por lo cuál al realizar todas las conbinaciones posibles de la matriz $n \times n$ y dividirlas por la $n$-dimensionalidad, se obtentra la misma matriz de varianzas y covarianzas.

```{r}
dd11 = dd_dot(d1,d1); 
dd12 = dd_dot(d1,d2); 
dd21 = dd_dot(d2,d1); 
dd22 = dd_dot(d2,d2); 
s11 =  dd11/3;  
s12 =  dd12/3; 
s21 =  dd21/3; 
s22 =  dd22/3; 
vars <- rbind(c(s11,s12),c(s21,s22));vars 
```

Note que esta matriz es la misma que la matriz de varianzas y covarianzas.

```{r}
Sn = ((nrow(x)-1)/nrow(x)) * cov(x)
```

Adicionalmente, sabemos que $r_{xy}=\frac{s_{xy}}{\sqrt{r_{xx}}\cdot\sqrt{r_{yy}}}$. Por lo cuál podemos reemplazar los valores obtenidos anteriormente para realizar la matriz:

```{r}
r11 =  s11/(sqrt(s11)*sqrt(s11));  
r12 =  s12/(sqrt(s11)*sqrt(s22));  
r21 =  s21/(sqrt(s22)*sqrt(s11));   
r22 =  s22/(sqrt(s22)*sqrt(s22));   
vars <- rbind(c(r11,r12),c(r21,r22));vars 
```

Note que esta matriz es la misma que la matriz de correlación.

```{r}
cor(x)
```

## Varianza Generalizada

La varianza de la muestra se define como:

```{r}

X <- rbind(c(-1,3,-2),c(2,4,2),c(5,2,3)); X
S = var(X) ; S
round(det(S))

```

## Varianza Generalizada Determinada por |R|

La multiplicacion de sii*sjj*...*snn (|R|) = |S|

```{r}

x1 = c(42,52,48,58);
x2 = c(4,5,4,3);
X <- cbind(x1,x2);X

S = var(X) ; S
det(S)

R <-cor(X); R
det_R = det(R); det_R

det_S = 1;
for (i in 1:ncol(X)){
  det_S = det_S*S[i,i];
}

det_S = det_S * det(R); det_S

```

# La Distribución Normal Multivariada.

## Evaluando la Normalidad de una Distribución Univariada Marginal

Diagramas de puntos para n<=25. Histogramas para n>25.

```{r} 
knitr::include_graphics("Uno.png") 
```

# Pruebas de Hipotenusa

## Estimador de maxima verosimilitud

Para encontrar los estimadores de máxima verosimilitud de una población normal bivariada a partir de una muestra aleatoria X, necesitamos estimar la media y la matriz de covarianza de la distribución.

La función de densidad de probabilidad para una población normal bivariada con media $\boldsymbol{\mu} = (\mu_1, \mu_2)$ y matriz de covarianza $\boldsymbol{\Sigma}$ es:

$$f(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{2\pi|\boldsymbol{\Sigma}|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$

Donde $\mathbf{x} = (x_1, x_2)$ es una variable aleatoria bivariada.

Para encontrar los estimadores de máxima verosimilitud de $\boldsymbol{\mu}$ y $\boldsymbol{\Sigma}$ a partir de la muestra X, primero necesitamos calcular la función de verosimilitud. La función de verosimilitud para una muestra aleatoria bivariada es:

$$L(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{x}_1, \dots, \mathbf{x}_n) = \prod_{i=1}^n f(\mathbf{x}_i; \boldsymbol{\mu}, \boldsymbol{\Sigma})$$

Tomando el logaritmo de la función de verosimilitud, obtenemos:

$$\log L(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{x}_1, \dots, \mathbf{x}_n) = -\frac{n}{2}\log(2\pi) -\frac{1}{2}\sum_{i=1}^n\log|\boldsymbol{\Sigma}| -\frac{1}{2}\sum_{i=1}^n(\mathbf{x}_i-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_i-\boldsymbol{\mu})$$

Podemos encontrar los estimadores de máxima verosimilitud para $\boldsymbol{\mu}$ y $\boldsymbol{\Sigma}$ maximizando esta función de log-verosimilitud.

Para estimar $\boldsymbol{\mu}$, necesitamos maximizar la función de log-verosimilitud con respecto a $\boldsymbol{\mu}$. Tomando la derivada de la función de log-verosimilitud con respecto a $\boldsymbol{\mu}$ e igualando a cero, obtenemos:

$$\hat{\boldsymbol{\mu}} = \frac{1}{n}\sum_{i=1}^n\mathbf{x}_i$$

Este es el estimador de máxima verosimilitud para la media de la población.

Continuando con la derivación, después de algunas manipulaciones matemáticas, podemos obtener la expresión para el estimador de máxima verosimilitud de $\boldsymbol{\Sigma}$:

$$\hat{\boldsymbol{\Sigma}} = \frac{1}{n}\sum_{i=1}^n(\mathbf{x}_i-\hat{\boldsymbol{\mu}})(\mathbf{x}_i-\hat{\boldsymbol{\mu}})^\mathrm{T}$$

Este es el estimador de máxima verosimilitud para la matriz de covarianza de la población.

En este ejemplo, el estimador de máxima verosimilitud para la media es $\hat{\boldsymbol{\mu}} = (4.0, 6.0)$ y el estimador de máxima verosimilitud para la matriz de covarianza es:

$$\hat{\boldsymbol{\Sigma}} = \begin{bmatrix} 0.6667 & 0.3333 \ 0.3333 & 1.3333 \end{bmatrix}$$

Para calcular estos estimadores en R, podemos utilizar las siguientes líneas de código:

```{r}

### Likehood con muestra
X = matrix(c(3, 4, 5, 4, 6, 4, 7, 7), ncol=2);
n = nrow(X);
mu_hat = colMeans(X);
Sigma_hat = cov(X); Sigma_hat

```

## T2 la distribución que no es T-student.

```{r}

X = cbind(c(2,8,6,8), c(12,9,9,10));X
mu_0 = c(7,11);mu_0

T2 = function (X, mu_0) { # Esto es la T^2
  x_bar = colMeans(X);
  S = cov(X);
  
  n = nrow(X);
  return(n * t(x_bar - mu_0) %*% solve(S) %*% (x_bar - mu_0));
}

F_val = function (X, mu_0, alpha = 0.05) { # Esto es fisher
  p = length(mu_0);
  n = nrow(X)
  
  df_num = p;
  df_denom = n - p;
  
  return(qf(1 - alpha, df_num, df_denom));
}


T2_F = function(X, mu_0, alpha = 0.05) { # Una vaiana rara que utiliza fisher y maso la T^2
  p = length(mu_0);
  n = nrow(X);
  
  df_num = (n - 1) * p;
  df_denom = n - p;
  return(df_num / df_denom * F_val(X, mu_0, alpha));
}

# ACA SE CAMBIA EL ALPHAAAAAA
T2_test = function(X, mu_0, alpha = 0.05) { # T2 debe ser menor que T2_F para aceptar
  return(T2(X, mu_0) < T2_F(X, mu_0, alpha)); # Si retorna true, se acepta mu_0. 
}


T2_test(X, mu_0)
```


## Wili Wonkas Lambda


```{r}
# Preferir este. Este es solo Lambda
wilks_lambda_1 = function(X, mu_0) {
  n = nrow(X);
  p = length(mu_0);
  x_bar = colMeans(X);
  
  S = matrix(0, ncol = p, nrow = p);
  for (i in 1:n) {
    si = X[i, ] - x_bar;
    S = S + si %*% t(si);
  }
  
  B = matrix(0, ncol = p, nrow = p);
  for (i in 1:n) {
    bi = X[i, ] - mu_0;
    B = B + bi %*% t(bi);
  }
  
  W = det(S) / det(B);
  
  return(W ^ (n / 2)) # Dividir por 4 si necesitan lambda^(1/2) para la prueba
} 

# Este es Lambda ^(2/n)
wilks_lambda_2 = function(X, mu_0) {
  t2 = T2(X, mu_0);
  n = nrow(X);
  
  return((1 + t2 / (n - 1)) ^ -1)
}

X = cbind(c(2,8,6,8), c(12,9,9,10));X
mu_0 = c(7,11);mu_0

wilks_lambda_1(X,mu_0)

wilks_lambda_1(X,mu_0)^(2/nrow(X))

lams = wilks_lambda_2(X, mu_0); lams

# Reemplazar este lambda por el alfa y comparar lo que da

T2_test(X, mu_0)
T2_test(X, mu_0,lams)

```

## Ejes de Confianza - Major Axses

```{r}
Z = cars;
h0 = c(16,40);

sqrt_axes <-function(Z,ho,i){
  p = length(h0);
  n = nrow(Z);
  einz = eigen(cov(Z));
  return(sqrt(einz$values[i])*sqrt(((p*(n-1))/(n*(n-p)))*F_val(Z,h0)))
}

major_axes <- function(Z,h0){
  p = length(h0);
  n = nrow(Z);
  Z_bar = colMeans(Z); 
  einz = eigen(cov(Z));
  z1 = Z_bar + sqrt_axes(Z,h0,1) *einz$vectors[1];
  z2 =  Z_bar - sqrt_axes(Z,h0,1)*einz$vectors[1];
  return(c(z1,z2));
}

minor_axes <- function(Z,h0){
  p = length(h0);
  n = nrow(Z);
  Z_bar = colMeans(Z); 
  einz = eigen(cov(Z));
  z1 = Z_bar + sqrt_axes(Z,h0,2)*einz$vectors[2];
  z2 =  Z_bar - sqrt_axes(Z,h0,2)*einz$vectors[2];
  return(c(z1,z2));
}

major_axes(Z,h0)
minor_axes(Z,h0)
```

## Longitudes

```{r}
long_axes <-function(X,h0){
  einz = eigen(cov(X));
  n = nrow(Z);
  l1 = sqrt(einz$values[1])*sqrt((1/n)*F_val(Z,h0));
  l2 = sqrt(einz$values[2])*sqrt((1/n)*F_val(Z,h0));
  return(c(l1,l2))
}
long_axes(Z,h0)
```

# Graficar la elipse:


```{r}
library(plotrix)

# Modificar xlim y ylim para que la figura encaje.

make_hip <- function(Z,ho){
  p = length(h0);
  n = nrow(Z);
  angle = atan(eigen(cov(Z))$vectors[2,1]/eigen(cov(Z))$vectors[1,1])
  plot(0,pch='',ylab='',xlab='',xlim=c(0,20),ylim=c(0,60))
  draw.ellipse(x=colMeans(Z)[1],y=colMeans(Z)[2],a=sqrt_axes(Z,ho,1),b=sqrt_axes(Z,h0,2),angle=angle,deg=FALSE)
  
}

make_hip(Z,h0)

```

# Graficas

### Graficar en $R^3$ (Plot3D)

```{r}
library(plot3D)

x <- c(5, 1, 3)
y <- c(-1, 3, 1)
origin <- c(0,0,0)

x0 = c(0, 0)
y0 = c(0, 0)
z0 = c(0, 0)
cols <- c('red','blue')

x1 = c(5, -1)
y1 = c(1, 3)
z1 = c(3, 1)

arrows3D(x0, y0, z0, x1, y1, z1,  col = cols, lwd = 2, d = 5,  
         main = "Arrows 3D", bty ="g", ticktype = "detailed", xlim = c(0,6), 
         ylim = c(0,4), zlim = c(0,4))
points3D(x0, y0, z0, add = TRUE, col="darkred", 
         colkey = FALSE, pch = 19, cex = 1)
text3D(x1, y1, z1, c("X", "Y"), colvar = NULL, col = cols, add=TRUE, colkey = FALSE)
```

### Boxplot

`boxplot(olive$oleic ~ olive$region, xlab = "Region", ylab = "Oleic")`

```{r}

packages = c("dslabs", "MASS", "scatterplot3d", "car")
## Se cargan o se instalan y cargan
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)


boxplot(olive$oleic ~ olive$region, xlab = "Region", ylab = "Oleic")

a = mtcars;
a_num = subset(a, select = c(-cyl, -vs, -am, -gear, -carb));
a_num_norm = as.data.frame(scale(a_num));
boxplot(a_num_norm)
```

### Histograma

```{r}
sapply(colnames(a), function(columna) {
values = unlist(a[columna]);
breaks = seq(min(values), max(values), length.out = 6);
hist(
x = values,
breaks = breaks,
main = paste("Histograma de ", columna)
);
})
```

### Scatterplot

```{r}
olive_subset = subset(olive, select = c(oleic, linoleic, linolenic))
scatterplotMatrix(~ +., data = olive_subset, diagonal = list(method = "boxplot"))
```

Fin.