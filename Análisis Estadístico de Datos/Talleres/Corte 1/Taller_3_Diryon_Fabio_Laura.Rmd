---
title: "Taller 3 AED."
author: "Diryon Mora, Fabio Rizo y Laura Gonzalez."
date: "2023-03-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Primer Punto.

#### Find the maximum likelihood estimates of the 2 × 1 mean vector µ and the 2 × 2 covariance matrix Σ based on the random sample from a bivariate normal population.

```{r}
X = matrix(c(3, 4, 5, 4, 6, 4, 7, 7), ncol=2);X
```
Para encontrar los estimadores de máxima verosimilitud de una población normal bivariada a partir de una muestra aleatoria X, necesitamos estimar la media y la matriz de covarianza de la distribución.

La función de densidad de probabilidad para una población normal bivariada con media $\boldsymbol{\mu} = (\mu_1, \mu_2)$ y matriz de covarianza $\boldsymbol{\Sigma}$ es:

$$f(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{2\pi|\boldsymbol{\Sigma}|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$

Donde $\mathbf{x} = (x_1, x_2)$ es una variable aleatoria bivariada.

Para encontrar los estimadores de máxima verosimilitud de $\boldsymbol{\mu}$ y $\boldsymbol{\Sigma}$ a partir de la muestra X, primero necesitamos calcular la función de verosimilitud. La función de verosimilitud para una muestra aleatoria bivariada es:

$$L(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{x}_1, \dots, \mathbf{x}_n) = \prod_{i=1}^n f(\mathbf{x}_i; \boldsymbol{\mu}, \boldsymbol{\Sigma})$$

Tomando el logaritmo de la función de verosimilitud, obtenemos:

$$\log L(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{x}_1, \dots, \mathbf{x}_n) = -\frac{n}{2}\log(2\pi) -\frac{1}{2}\sum_{i=1}^n\log|\boldsymbol{\Sigma}| -\frac{1}{2}\sum_{i=1}^n(\mathbf{x}_i-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_i-\boldsymbol{\mu})$$

Podemos encontrar los estimadores de máxima verosimilitud para $\boldsymbol{\mu}$ y $\boldsymbol{\Sigma}$ maximizando esta función de log-verosimilitud.

Para estimar $\boldsymbol{\mu}$, necesitamos maximizar la función de log-verosimilitud con respecto a $\boldsymbol{\mu}$. Tomando la derivada de la función de log-verosimilitud con respecto a $\boldsymbol{\mu}$ e igualando a cero, obtenemos:

$$\hat{\boldsymbol{\mu}} = \frac{1}{n}\sum_{i=1}^n\mathbf{x}_i$$

Este es el estimador de máxima verosimilitud para la media de la población.

```{r}
mu_hat <- colMeans(X); mu_hat
```

Continuando con la derivación, después de algunas manipulaciones matemáticas, podemos obtener la expresión para el estimador de máxima verosimilitud de $\boldsymbol{\Sigma}$:

$$\hat{\boldsymbol{\Sigma}} = \frac{1}{n}\sum_{i=1}^n(\mathbf{x}_i-\hat{\boldsymbol{\mu}})(\mathbf{x}_i-\hat{\boldsymbol{\mu}})^\mathrm{T}$$

Este es el estimador de máxima verosimilitud para la matriz de covarianza de la población.


```{r}
n = nrow(X);
Sigma_hat <-(n - 1) / n * cov(X); Sigma_hat
```

## Segundo Punto.

#### Definimos las funciones:

```{r}
T2 = function (X, mu_0) {
  x_bar = colMeans(X);
  S = cov(X);
  n = nrow(X);
  return(n * t(x_bar - mu_0) %*% solve(S) %*% (x_bar - mu_0));
}

F_val = function (n, p, alpha = 0.05) {
  return(qf(1 - alpha, df1 = p, df2 = n - p));
}

T2_F = function(n, p, alpha = 0.05) {
  if (n - p >= 30) {
    return (qchisq(1 - alpha, df = p))
  }
  num =  p * (n - 1);
  denom = n - p;
  return((num / denom )* F_val(n, p, alpha));
}

T2_test = function(X, mu_0, alpha = 0.05) {
  n = nrow(X) 
  p = ncol(X)
  return(T2(X, mu_0) < T2_F(n, p, alpha)); 
  # T2 debe ser menor que T2_F para aceptar
  # Si retorna true, se acepta mu_0.
}

sqrt_wilks_lambda = function(X, mu_0) {
  t2 = T2(X, mu_0);
  n = nrow(X);
  return((1 + t2 / (n - 1)) ^ -1)
}

wilks_lambda = function(X, mu_0) {
  n = nrow(X);
  p = length(mu_0);
  x_bar = colMeans(X);
  S = matrix(0, ncol = p, nrow = p);
  for (i in 1:n) {
    si = X[i, ] - x_bar;
    S = S + si %*% t(si);
  }
  B = matrix(0, ncol = p, nrow = p);
  for (i in 1:n) {
    bi = X[i, ] - mu_0;
    B = B + bi %*% t(bi);
  }
  W = det(S) / det(B);
  return(W ^ (n / 2))
}
```

####  Using the data:

```{r}
X = matrix(c(2, 8, 6, 8, 12, 9, 9, 10), ncol=2);X
```

#### a) Evaluate  $\boldsymbol{T^2}$, for testing $\boldsymbol{H_0: \mu ' = [7, 11]}$, using the data.

```{r}
mu_0 = c(7, 11);
n = nrow(X)
p = ncol(X)
alpha = 0.05
T2(X, mu_0)
```

#### b) Specify the distribution of $\boldsymbol{T^2}$ for the situation in (a).

Para este caso:
$$T^2 = \frac{(n-1)\cdot p}{(n-p)} F_{p,n-p} = \frac{(4-1)\cdot2}{4-2} F_{2,4-2} = 3 F_{2,2}(\alpha ) $$

Asumiendo un $\boldsymbol{\alpha} = 0.05$.

Se tiene un valor de:

```{r}
T2_F(n, p, alpha)
```

#### c) Using (a) and (b), test $\boldsymbol{H_0}$ H0 at the $\boldsymbol{\alpha = 0.05}$ level. What conclusion do you reach?

```{r}
T2_test(X, mu_0, alpha) ### Como es True, se acepta mu_0
```

Note que:

$$ T^2 = 13.64 < 57 = 3 F_{2,2}(0.05)$$

Por lo cuál, no se rechaza H0 (se acepta) con una confianza del 95%.

#### d) Determine $\boldsymbol{Λ}$.

La formula para $\boldsymbol{Λ}$ es:

$$
Λ = \frac{|\hat{\sum}|}{|\hat{{\sum}}_0|}
$$

Donde $\boldsymbol{\hat{\sum}}$ es:

$$
\hat{\sum} = \frac{1}{n}\sum_{j=1}^{n}(X_j-\bar{X}){(X_j-\bar{X})}^{'}
$$

Ademas $\boldsymbol{\hat{{\sum}}_0}$ es:

$$
\hat{{\sum}}_0= \frac{1}{n}\sum_{j=1}^{n}(X_j-\mu_0){(X_j-\mu_0)}^{'}
$$
Así se tiene el valor de Λ:

```{r}
alpha_wilk = wilks_lambda(X, mu_0);alpha_wilk
```

#### e) Using the Wilk’s Lambda (d), calculate $\boldsymbol{T^2}$.

```{r}
T2_F(n, p, alpha_wilk)
```
$$T^2 = 3 F_{2,2}(\alpha ) = 3 F_{2,2}(0.03251814 ) =  89.2562 $$

```{r}
T2_test(X, mu_0, alpha_wilk) ### Como es True, se acepta mu_0
```
Por lo cuál, no se rechaza H0 (se acepta) con una confianza del 95%.

## Tercer Punto.

#### Harry Roberts, a naturalist for the Alaska Fish and Game department, studies grizzly bears with the goal of maintaining a healthy population. Measurements on n = 61 bears provided the following summary statistics:

Covariance matrix $\boldsymbol{S}$:

```{r}
S <- matrix(c(3266.46, 1343.97, 731.54, 1175.50, 162.68, 238.37,
              1343.97, 721.91, 324.25, 537.35, 80.17, 117.73,
              731.54, 324.25, 179.28, 281.17, 39.15, 56.80,
              1175.50, 537.35, 281.17, 474.98, 63.73, 94.85,
              162.68, 80.17, 39.15, 63.73, 9.95, 13.88,
              238.37, 117.73, 56.80, 94.85, 13.88, 21.26),
            nrow = 6, byrow = TRUE);S
```
Sample mean $\boldsymbol{\bar{x}}$:
```{r}
x_bar = c(95.52, 164.38, 55.69, 93.39, 17.98, 31.13);x_bar
S1 = matrix(c(S[1, 1], S[1, 4], S[4, 1], S[4, 4]), ncol = 2)
x_bar1 = matrix(c(x_bar[1], x_bar[4]), ncol = 2) # weight y girth
n = 61;
p = length(x_bar)
A = diag(1, nrow = p)
alpha = 0.05;
```
#### Definimos las funciones:

```{r}
simultaneous_ci <- function(a, x_bar, S, n, p, alpha) {
  right = sqrt((T2_F(n, p, alpha) / n) * (t(a) %*% S %*% a));
  left = t(a) %*% x_bar; 
  return(c(left + right, left - right))
}

bonferroni_ci = function(a, x_bar, S, n, m, alpha) {
  t_val = qt(1 - (alpha / (2*m)), df = n - 1)
  right = t_val * sqrt((t(a) %*% S %*% a) / n)
  left = t(a) %*% x_bar
  return(c(left + right, left - right))
}

graficar_ellipse <- function(S, x_bar, n, p, alpha, range){
  eig_S = eigen(S) ;
  eig_vec = eig_S$vectors;
  eig_val = eig_S$values;
  max_eig_val_ind = which.max(eig_val)
  min_eig_val_ind = which.min(eig_val)
  max_eig_val = eig_val[max_eig_val_ind] 
  max_eig_vec = eig_vec[, max_eig_val_ind];
  min_eig_val = eig_val[min_eig_val_ind]
  min_eig_vec = eig_vec[,min_eig_val_ind]
  angle = atan2(max_eig_vec[2], max_eig_vec[1])
  if (angle < 0) {
    angle = angle + 2 * pi
  }
  print("Angulo: ")
  print(angle)
  t2f_val = sqrt(T2_F(n, p, alpha) / n )
  a = t2f_val * sqrt(max_eig_val)
  print("Eje x")
  print(a)
  b = t2f_val * sqrt(min_eig_val)
  print("Eje y")
  print(b)
  angle_grid = seq(0, 2 * pi, length.out = 100);
  ellipse_x_r = a * cos(angle_grid);
  ellipse_y_r = b * sin(angle_grid);
  rot = matrix(c(
    cos(angle), sin(angle),
    -sin(angle), cos(angle)
  ), ncol = 2, byrow = TRUE);
  r_ellipse = matrix(c(ellipse_x_r, ellipse_y_r), ncol = 2) %*% rot;
  x = r_ellipse[, 1] + x_bar[1]
  y = r_ellipse[, 2] + x_bar[2]
  df = data.frame(x = x, y = y)
  r = range
  return(
    ggplot(df, aes(x, y)) + 
    geom_path() + 
    xlim(min(x) - r, max(x) + r) + 
    ylim(min(y) - r, max(y) + r) 
  )
}

graficar_intervalos = function(intervals, linetype = "dashed", color = "black") {
  x = rep(intervals[, 1], each = 2)
  y = c(intervals[, 2], rev(intervals[, 2]))
  df = data.frame(x = x, y = y)
  return(geom_polygon(df, mapping = aes(x, y, linetype = linetype), fill = NA, color = color, linewidth = 1))
}

```

#### a) Obtain the large sample 95% simultaneous confidence intervals for the six population mean body measurements.

```{r}
sim_intervals = matrix(0, ncol = p, nrow = 2);
for (i in 1:p) {
  a = A[i, ];
  sim_intervals[, i] = simultaneous_ci(a, x_bar, S, n, p, alpha)
}
sim_intervals

T2_F(n, p, alpha)
```
Los valores corresponden de izquierda a derecha a: Weight, Body Length, Neck, Girth, Head Length y Head Width.

#### b) Obtain the large sample 95% confidence ellipse for mean weight and mean girth.

La elipse de confianza del 95% para el weigth y el girth, tendra un centro $\boldsymbol{\bar{x} = (95.52, 93.39)$:

Además:

```{r}
graph = graficar_ellipse(S1, x_bar1, n, p, alpha = 0.05, range = 5);
graph + xlab("Weight") +  ylab("Girth") + geom_point(x=95.52, y=93.39)
```

#### c) Obtain the 95% Bonferroni confidence intervals for the six means in Part a.

```{r}
m = 6
bon_intervals = matrix(0, ncol = p, nrow = 2);
for (i in 1:p) {
  a = A[i, ];
  bon_intervals[, i] = bonferroni_ci(a, x_bar, S, n, m, alpha)
}
bon_intervals
```
Los valores corresponden de izquierda a derecha a: Weight, Body Length, Neck, Girth, Head Length y Head Width.

#### d) Refer to Part b. Construct the 95% Bonferroni confidence rectangle for the mean weight and mean girth using m = 6. Compare this rectangle with the confidence ellipse in Part b.

```{r}
bon_intervals1 = matrix(c(bon_intervals[, 1], bon_intervals[, 4]), ncol = 2);
sim_intervals1 = matrix(c(sim_intervals[, 1], sim_intervals[, 4]), ncol = 2);
graph + 
  graficar_intervalos(bon_intervals1, linetype = "Bonferroni") +
  graficar_intervalos(sim_intervals1, linetype = "Simultáneos") +
  scale_linetype_manual(name = "Intervalos", values = c("Bonferroni" = "dashed", "Simultáneos" = "dotted")) +
  xlab("Weight") +
  ylab("Girth")+ geom_point(x=95.52, y=93.39)
```

La elipse de confianza del 95% es más informativa y pequeña que el rectángulo de Bonferroni del 95% debido a la fuerte correlación positiva entre el peso (X1) y (X4).
